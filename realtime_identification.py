import os
import pyaudio
import wave
import time
import queue
import threading
from io import BytesIO
from openai import OpenAI
import webrtcvad
import torch
import numpy as np
from pyannote.audio import Pipeline
from speechbrain.inference import SpeakerRecognition
from huggingface_hub import login
from scipy.spatial.distance import cosine
from dotenv import load_dotenv
import torchaudio
import soundfile as sf
from datetime import datetime

load_dotenv()
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
# Hugging Face ã«ãƒ­ã‚°ã‚¤ãƒ³
login(token=os.getenv("HUGGINGFACE_TOKEN"))

# è©±è€…è­˜åˆ¥ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰
diarization_pipeline = Pipeline.from_pretrained("pyannote/speaker-diarization-3.1", use_auth_token=os.getenv("HUGGINGFACE_TOKEN"))

# å¿…è¦ãªã‚‰ GPU ã‚’ä½¿ç”¨
if torch.cuda.is_available():
    diarization_pipeline.to(torch.device("cuda"))

embedding_model = SpeakerRecognition.from_hparams(source="speechbrain/spkrec-ecapa-voxceleb", savedir="tmp")

# éŒ²éŸ³è¨­å®š
SAMPLE_RATE = 16000  # Whisperã®æ¨å¥¨ã‚µãƒ³ãƒ—ãƒ«ãƒ¬ãƒ¼ãƒˆ
CHUNK = 160  # 16-bit PCM (1ã‚µãƒ³ãƒ—ãƒ«=2ãƒã‚¤ãƒˆ) â†’ 160ã‚µãƒ³ãƒ—ãƒ« = 320ãƒã‚¤ãƒˆ
FORMAT = pyaudio.paInt16  # 16-bit PCM
CHANNELS = 1  # ãƒ¢ãƒãƒ©ãƒ«
VAD_MODE = 3  # 0(æ•æ„Ÿ) - 3(éˆæ„Ÿ) (webrtcVADã§æ²ˆé»™æ¤œçŸ¥)
SILENCE_DURATION = 1  # ç„¡éŸ³ãŒç¶šã„ãŸã‚‰é€ä¿¡ã™ã‚‹ç§’æ•°
MIN_SPEAKERS = 1  # æœ€å°ã®è©±è€…æ•°
MAX_SPEAKERS = 3  # æœ€å¤§ã®è©±è€…æ•°
NUM_SPEAKERS = 2  # ç¢ºå®šè©±è€…æ•°
SIMILARITY_THRESHOLD = 0.0  # æ—¢å­˜ã®è©±è€…ã¨ã®é¡ä¼¼åº¦ã®é–¾å€¤

# éŸ³å£°ã‚¢ã‚¯ãƒ†ã‚£ãƒ“ãƒ†ã‚£æ¤œå‡º (VAD)
vad = webrtcvad.Vad(VAD_MODE)

# æ—¢å­˜ã®è©±è€…æƒ…å ± (è©±è€…å: embedding)
known_speakers = {}

# éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã®ã‚­ãƒ¥ãƒ¼ï¼ˆéŒ²éŸ³ã‚¹ãƒ¬ãƒƒãƒ‰ â†’ å‡¦ç†ã‚¹ãƒ¬ãƒƒãƒ‰ã¸æ¸¡ã™ï¼‰
audio_queue = queue.Queue()

# ä¼šè©±å±¥æ­´ãƒ­ã‚°
conversation_log = []


def is_speech(frame, sample_rate):
    """VAD ã‚’ä½¿ã£ã¦éŸ³å£°ã‹ç„¡éŸ³ã‹ã‚’åˆ¤å®š"""
    if len(frame) not in [160, 320, 480]:  # ãƒ•ãƒ¬ãƒ¼ãƒ ã‚µã‚¤ã‚ºã‚’ãƒã‚§ãƒƒã‚¯
        print(f"âš ï¸ ãƒ•ãƒ¬ãƒ¼ãƒ ã‚µã‚¤ã‚ºãŒç•°å¸¸: {len(frame)} ãƒã‚¤ãƒˆ (è¨±å®¹å€¤: 160, 320, 480)")
        return False
    return vad.is_speech(frame, sample_rate)


def extract_embedding(audio_input):
    """éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰è©±è€…ã®ç‰¹å¾´é‡ï¼ˆembeddingï¼‰ã‚’å–å¾—"""

    if isinstance(audio_input, str):  # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã®å ´åˆ
        with open(audio_input, "rb") as f:
            audio_buffer = BytesIO(f.read())
    else:  # æ—¢ã« `BytesIO` ã®å ´åˆ
        audio_buffer = audio_input

    audio_buffer.seek(0)

    with wave.open(audio_buffer, "rb") as wf:
        audio_data = np.frombuffer(wf.readframes(wf.getnframes()), dtype=np.int16)

    # å…¥åŠ›ãŒæ¥µç«¯ã«çŸ­ã„å ´åˆã¯å‡¦ç†ã‚’ã‚¹ã‚­ãƒƒãƒ—
    if len(audio_data) < 1600:  # ç´„0.1ç§’æœªæº€ã®ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¹ã‚­ãƒƒãƒ—
        print("âš ï¸ éŸ³å£°ãƒ‡ãƒ¼ã‚¿ãŒçŸ­ã™ãã‚‹ãŸã‚ã€ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
        return None

    # ãƒ¢ãƒ‡ãƒ«ã«å…¥åŠ›ã§ãã‚‹å½¢å¼ã«å¤‰æ›
    audio_tensor = torch.tensor(audio_data, dtype=torch.float32).unsqueeze(0) / 32768.0

    # Embeddingï¼ˆç‰¹å¾´é‡ï¼‰ã‚’å–å¾—
    with torch.no_grad():
        embedding = embedding_model.encode_batch(audio_tensor).squeeze(0)

    return embedding


def register_reference_speaker(speaker_name, reference_audio_path):
    """ å‚è€ƒéŸ³å£°ã‹ã‚‰è©±è€…åŸ‹ã‚è¾¼ã¿ã‚’ä½œæˆã—ç™»éŒ² """
    reference_embedding = extract_embedding(reference_audio_path)
    known_speakers[speaker_name] = reference_embedding


def identify_speaker(audio_buffer):
    """è©±è€…è­˜åˆ¥ï¼šæ–°ã—ã„è©±è€…ãªã‚‰ç™»éŒ²ã€æ—¢å­˜ã®è©±è€…ãªã‚‰ãƒ©ãƒ™ãƒ«ä»˜ã‘"""
    global known_speakers

    new_embedding = extract_embedding(audio_buffer)
    if new_embedding is None:
        return "æœªè­˜åˆ¥"

    new_embedding = new_embedding.flatten()

    best_match = None
    best_score = -1.0

    # å‚è€ƒéŸ³å£°ã‚’ä½¿ã£ã¦è©±è€…ã‚’ç‰¹å®š
    for speaker, emb in known_speakers.items():
        similarity = 1 - cosine(new_embedding, emb.flatten())
        print(f"ğŸ” é¡ä¼¼åº¦ï¼ˆ{speaker}ï¼‰: {similarity:.2f}")
        if similarity > SIMILARITY_THRESHOLD and similarity > best_score:  # SIMILARITY_THRESHOLD ä»¥ä¸‹ãªã‚‰æ–°è¦ç™»éŒ²ã€‚ä»¥ä¸Šãªã‚‰æ—¢å­˜è©±è€…ã‹ã‚‰é¡ä¼¼åº¦ãŒæœ€ã‚‚å¤§ãã„ã‚‚ã®ã‚’é¸æŠ
            best_score = similarity
            best_match = speaker

    if best_match:
        return best_match

    # æ–°ã—ã„è©±è€…ã¨ã—ã¦ç™»éŒ²
    new_speaker_id = f"è©±è€…_{len(known_speakers) + 1}"
    known_speakers[new_speaker_id] = new_embedding
    return new_speaker_id


def diarize_audio(audio_buffer):
    """è©±è€…åˆ†é›¢ã‚’é©ç”¨ã—ã¦ã€è©±è€…ã”ã¨ã®éŸ³å£°ã‚’åˆ†é›¢"""
    audio_buffer.seek(0)
    waveform, sample_rate = torchaudio.load(audio_buffer)
    diarization = diarization_pipeline({"waveform": waveform, "sample_rate": sample_rate}, min_speakers=MIN_SPEAKERS, max_speakers=MAX_SPEAKERS)
    # diarization = diarization_pipeline({"waveform": waveform, "sample_rate": sample_rate}, num_speakers=NUM_SPEAKERS)

    # ğŸ”¹ ç™ºè©±é †ã‚’ä¿æŒã™ã‚‹ãŸã‚ã€ãƒªã‚¹ãƒˆã‚’ä½¿ç”¨
    speaker_timeline = []

    for turn, _, speaker in diarization.itertracks(yield_label=True):
        speaker_timeline.append((speaker, turn.start, turn.end))

    return speaker_timeline, waveform, sample_rate


def record_audio():
    """
    éŸ³å£°éŒ²éŸ³ã‚¹ãƒ¬ãƒƒãƒ‰
    éŸ³å£°ãŒç„¡éŸ³ã«ãªã£ãŸã‚‰ã€éŸ³å£°ã‚’ã‚­ãƒ¥ãƒ¼ã«è¿½åŠ 
    """
    print("éŒ²éŸ³ä¸­...")
    p = pyaudio.PyAudio()
    stream = p.open(format=FORMAT, channels=CHANNELS, rate=SAMPLE_RATE, input=True, frames_per_buffer=CHUNK)

    frames = []
    silence_start_time = None
    has_voice = False

    try:
        while True:
            try:
                data = stream.read(CHUNK, exception_on_overflow=False)
            except IOError:
                print("âš ï¸ éŸ³å£°ãƒ‡ãƒ¼ã‚¿ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚ç„¡éŸ³ã¨ã—ã¦å‡¦ç†ã—ã¾ã™ã€‚")
                data = b"\x00" * CHUNK

            frames.append(data)

            if is_speech(data, SAMPLE_RATE):
                has_voice = True
                silence_start_time = None
            else:
                if silence_start_time is None:
                    silence_start_time = time.time()
                elif time.time() - silence_start_time >= SILENCE_DURATION:
                    if has_voice:
                        # print("æ²ˆé»™ã‚’æ¤œçŸ¥ï¼éŸ³å£°ã‚’ã‚­ãƒ¥ãƒ¼ã«è¿½åŠ ã—ã¾ã™...")
                        audio_queue.put(frames)
                    frames = []
                    has_voice = False
    except KeyboardInterrupt:
        print("éŒ²éŸ³çµ‚äº†")

    stream.stop_stream()
    stream.close()
    p.terminate()


def process_audio():
    """
    éŸ³å£°å‡¦ç†ã‚¹ãƒ¬ãƒƒãƒ‰
    éŸ³å£°ãŒã‚­ãƒ¥ãƒ¼ã«è¿½åŠ ã•ã‚Œã‚‹ãŸã³ã«ã€è©±è€…åˆ†é›¢ã¨æ–‡å­—èµ·ã“ã—ã‚’å®Ÿè¡Œ
    """
    while True:
        frames = audio_queue.get()
        if not frames:
            continue

        print("éŸ³å£°å‡¦ç†ä¸­...")
        audio_buffer = BytesIO()
        with wave.open(audio_buffer, "wb") as wf:
            wf.setnchannels(CHANNELS)
            wf.setsampwidth(2)
            wf.setframerate(SAMPLE_RATE)
            wf.writeframes(b"".join(frames))
        audio_buffer.seek(0)

        speaker_timeline, waveform, sample_rate = diarize_audio(audio_buffer)

        prev_speaker = None
        combined_audio_list = []
        current_audio = BytesIO()

        for speaker, start, end in speaker_timeline:
            start_sample = int(start * sample_rate)
            end_sample = int(end * sample_rate)
            segment_waveform = waveform[:, start_sample:end_sample]

            # ğŸ”¹ åŒã˜è©±è€…ã®ç™ºè©±ãªã‚‰éŸ³å£°ã‚’çµåˆ
            if prev_speaker == speaker:
                combined_audio_list.append(segment_waveform.numpy())
            else:
                # ğŸ”¹ è©±è€…ãŒå¤‰ã‚ã£ãŸã‚‰ã€ç›´å‰ã®è©±è€…ã®éŸ³å£°ã‚’å‡¦ç†
                if combined_audio_list:
                    combined_waveform = np.concatenate(combined_audio_list, axis=1)
                    sf.write(current_audio, combined_waveform.T, sample_rate, format="WAV")
                    current_audio.seek(0)

                    recognized_speaker = identify_speaker(current_audio)
                    if recognized_speaker == "æœªè­˜åˆ¥":
                        print("âš ï¸ è©±è€…ãŒè­˜åˆ¥ã§ããªã‹ã£ãŸãŸã‚ã€æ–‡å­—èµ·ã“ã—ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
                    else:
                        transcript = client.audio.transcriptions.create(
                            # model="whisper-1",
                            # model="gpt-4o-mini-transcribe",  # é€Ÿåº¦é‡è¦–
                            model="gpt-4o-transcribe",
                            file=("audio_segment.wav", current_audio, "audio/wav"),
                            language="ja"
                        )
                        print(f"[{recognized_speaker}] {transcript.text}")
                        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                        log_line = f"[{timestamp}] [{recognized_speaker}] {transcript.text}"
                        conversation_log.append(log_line)

                # ğŸ”¹ æ–°ã—ã„è©±è€…ã®ãŸã‚ã«ãƒªã‚»ãƒƒãƒˆ
                combined_audio_list = [segment_waveform.numpy()]
                current_audio = BytesIO()

            prev_speaker = speaker

        # ğŸ”¹ æœ€å¾Œã®è©±è€…ã®ç™ºè©±ã‚‚å‡¦ç†
        if combined_audio_list:
            combined_waveform = np.concatenate(combined_audio_list, axis=1)
            sf.write(current_audio, combined_waveform.T, sample_rate, format="WAV")
            current_audio.seek(0)

            recognized_speaker = identify_speaker(current_audio)
            if recognized_speaker == "æœªè­˜åˆ¥":
                print("âš ï¸ è©±è€…ãŒè­˜åˆ¥ã§ããªã‹ã£ãŸãŸã‚ã€æ–‡å­—èµ·ã“ã—ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
            else:
                transcript = client.audio.transcriptions.create(
                    # model="whisper-1",
                    # model="gpt-4o-mini-transcribe",  # é€Ÿåº¦é‡è¦–
                    model="gpt-4o-transcribe",
                    file=("audio_segment.wav", current_audio, "audio/wav"),
                    language="ja"
                )
                print(f"[{recognized_speaker}] {transcript.text}")
                timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                log_line = f"[{timestamp}] [{recognized_speaker}] {transcript.text}"
                conversation_log.append(log_line)


def save_conversation_log():
    if not conversation_log:
        print("ğŸ”• ä¼šè©±ãƒ­ã‚°ã¯ç©ºã§ã™ã€‚")
        return

    os.makedirs("logs", exist_ok=True)
    filename = datetime.now().strftime("logs/conversation6.txt")
    with open(filename, "w", encoding="utf-8") as f:
        for line in conversation_log:
            f.write(line + "\n")
    print(f"ğŸ’¾ ä¼šè©±ãƒ­ã‚°ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {filename}")


if __name__ == "__main__":
    register_reference_speaker("å°é‡å¯º", "static/audio/onodera_sample.wav")
    register_reference_speaker("ä½è—¤", "static/audio/sato_sample.wav")
    register_reference_speaker("ç”°ä¸­", "static/audio/tanaka_sample.wav")
    # while True:
    #     record_and_transcribe()
    threading.Thread(target=record_audio, daemon=True).start()
    threading.Thread(target=process_audio, daemon=True).start()

    try:
        while True:
            time.sleep(1)
    except KeyboardInterrupt:
        print("ğŸ›‘ çµ‚äº†ä¿¡å·ã‚’å—ã‘å–ã‚Šã¾ã—ãŸã€‚ãƒ­ã‚°ã‚’ä¿å­˜ã—ã¾ã™...")
        save_conversation_log()

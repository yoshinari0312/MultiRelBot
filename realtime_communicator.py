import os
import re
import pyaudio
import wave
import time
import queue
import threading
from io import BytesIO
from openai import OpenAI
import webrtcvad
import torch
import numpy as np
from pyannote.audio import Pipeline
from speechbrain.inference import SpeakerRecognition
from huggingface_hub import login
from scipy.spatial.distance import cosine
from dotenv import load_dotenv
import torchaudio
import soundfile as sf
from datetime import datetime
from session_manager import SessionManager
import socketio
socketio_cli = socketio.Client()

load_dotenv()
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
# Hugging Face ã«ãƒ­ã‚°ã‚¤ãƒ³
login(token=os.getenv("HUGGINGFACE_TOKEN"))

# è©±è€…è­˜åˆ¥ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰
diarization_pipeline = Pipeline.from_pretrained("pyannote/speaker-diarization-3.1", use_auth_token=os.getenv("HUGGINGFACE_TOKEN"))

# å¿…è¦ãªã‚‰ GPU ã‚’ä½¿ç”¨
if torch.cuda.is_available():
    diarization_pipeline.to(torch.device("cuda"))

embedding_model = SpeakerRecognition.from_hparams(source="speechbrain/spkrec-ecapa-voxceleb", savedir="tmp")

# æ—¥æœ¬èªæ–‡å­—ï¼ˆã²ã‚‰ãŒãªãƒ»ã‚«ã‚¿ã‚«ãƒŠãƒ»CJKçµ±åˆæ¼¢å­—ï¼‰ã®æ­£è¦è¡¨ç¾
_jp_regex = re.compile(r'[\u3040-\u309F\u30A0-\u30FF\u4E00-\u9FFF]')

# éŒ²éŸ³è¨­å®š
SAMPLE_RATE = 16000  # Whisperã®æ¨å¥¨ã‚µãƒ³ãƒ—ãƒ«ãƒ¬ãƒ¼ãƒˆ
CHUNK = 160  # 16-bit PCM (1ã‚µãƒ³ãƒ—ãƒ«=2ãƒã‚¤ãƒˆ) â†’ 160ã‚µãƒ³ãƒ—ãƒ« = 320ãƒã‚¤ãƒˆ
FORMAT = pyaudio.paInt16  # 16-bit PCM
CHANNELS = 1  # ãƒ¢ãƒãƒ©ãƒ«
VAD_MODE = 3  # 0(æ•æ„Ÿ) - 3(éˆæ„Ÿ) (webrtcVADã§æ²ˆé»™æ¤œçŸ¥)
SILENCE_DURATION = 0.5  # ç„¡éŸ³ãŒç¶šã„ãŸã‚‰é€ä¿¡ã™ã‚‹ç§’æ•°
MIN_SPEAKERS = 1  # æœ€å°ã®è©±è€…æ•°
MAX_SPEAKERS = 4  # æœ€å¤§ã®è©±è€…æ•°
# NUM_SPEAKERS = 3  # ç¢ºå®šè©±è€…æ•°
SIMILARITY_THRESHOLD = -10  # æ—¢å­˜ã®è©±è€…ã¨ã®é¡ä¼¼åº¦ã®é–¾å€¤
N_BATCH = 5  # ã‚»ãƒƒã‚·ãƒ§ãƒ³åˆ†å‰²åˆ¤å®šã®ãƒãƒƒãƒã‚µã‚¤ã‚º
ROBOT_UTTERANCE_REMAIN = 0  # ãƒ­ãƒœãƒƒãƒˆç™ºè©±å¾Œã«ãƒ­ãƒœãƒƒãƒˆè­˜åˆ¥ã‚’æœ‰åŠ¹ã«ã™ã‚‹æ®‹ã‚Šç™ºè©±æ•°

# éŸ³å£°ã‚¢ã‚¯ãƒ†ã‚£ãƒ“ãƒ†ã‚£æ¤œå‡º (VAD)
vad = webrtcvad.Vad(VAD_MODE)

# æ—¢å­˜ã®è©±è€…æƒ…å ± (è©±è€…å: embedding)
known_speakers = {}

# éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã®ã‚­ãƒ¥ãƒ¼ï¼ˆéŒ²éŸ³ã‚¹ãƒ¬ãƒƒãƒ‰ â†’ å‡¦ç†ã‚¹ãƒ¬ãƒƒãƒ‰ã¸æ¸¡ã™ï¼‰
audio_queue = queue.Queue()

# ä¼šè©±å±¥æ­´ãƒ­ã‚°
conversation_log = []

# === ã‚»ãƒƒã‚·ãƒ§ãƒ³ç®¡ç†ã®åˆæœŸåŒ– ===
session_manager = SessionManager()

recording_enabled = True

# ãƒ†ã‚­ã‚¹ãƒˆçµåˆç”¨ã®å¤‰æ•°
buffer_speaker = None
buffer_text = ""
buffer_time = None


# recording_enabledã‚’ã‚¹ã‚¿ãƒ¼ãƒˆãƒœã‚¿ãƒ³æŠ¼ã—ãŸã‚‰Trueã€ã‚¹ãƒˆãƒƒãƒ—ãƒœã‚¿ãƒ³æŠ¼ã—ãŸã‚‰Falseã«ã™ã‚‹
@socketio_cli.on("control")
def on_control(data):
    global recording_enabled
    recording_enabled = (data.get("action") == "start")
    print(f"ğŸ”” control: recording_enabled = {recording_enabled}")


def set_robot_count(n):
    global ROBOT_UTTERANCE_REMAIN
    ROBOT_UTTERANCE_REMAIN = n


def dec_robot_count():
    global ROBOT_UTTERANCE_REMAIN
    if ROBOT_UTTERANCE_REMAIN > 0:
        ROBOT_UTTERANCE_REMAIN -= 1


def get_robot_count():
    return ROBOT_UTTERANCE_REMAIN


def is_japanese(text: str) -> bool:
    return bool(_jp_regex.search(text))


def try_connect_socketio(url="http://localhost:8888", max_retries=10, interval_sec=2):
    """Flask Socket.IO ã‚µãƒ¼ãƒã¸ã®æ¥ç¶šã‚’ãƒªãƒˆãƒ©ã‚¤ã—ãªãŒã‚‰è©¦ã¿ã‚‹"""
    for attempt in range(1, max_retries + 1):
        try:
            socketio_cli.connect(url)
            print("âœ… Socket.IO ã«æ¥ç¶šã—ã¾ã—ãŸ")
            return
        except Exception as e:
            print(f"âš ï¸ Socket.IO æ¥ç¶šå¤±æ•— (è©¦è¡Œ {attempt}/{max_retries}): {e}")
            time.sleep(interval_sec)
    print("âŒ Socket.IO ã«æ¥ç¶šã§ãã¾ã›ã‚“ã§ã—ãŸã€‚Web UI æ©Ÿèƒ½ã¯ç„¡åŠ¹ã«ãªã‚Šã¾ã™ã€‚")


def is_speech(frame, sample_rate):
    """VAD ã‚’ä½¿ã£ã¦éŸ³å£°ã‹ç„¡éŸ³ã‹ã‚’åˆ¤å®š"""
    if len(frame) not in [160, 320, 480]:  # ãƒ•ãƒ¬ãƒ¼ãƒ ã‚µã‚¤ã‚ºã‚’ãƒã‚§ãƒƒã‚¯
        print(f"âš ï¸ ãƒ•ãƒ¬ãƒ¼ãƒ ã‚µã‚¤ã‚ºãŒç•°å¸¸: {len(frame)} ãƒã‚¤ãƒˆ (è¨±å®¹å€¤: 160, 320, 480)")
        return False
    return vad.is_speech(frame, sample_rate)


def extract_embedding(audio_input):
    """éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰è©±è€…ã®ç‰¹å¾´é‡ï¼ˆembeddingï¼‰ã‚’å–å¾—"""

    if isinstance(audio_input, str):  # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã®å ´åˆ
        with open(audio_input, "rb") as f:
            audio_buffer = BytesIO(f.read())
    else:  # æ—¢ã« `BytesIO` ã®å ´åˆ
        audio_buffer = audio_input

    audio_buffer.seek(0)

    with wave.open(audio_buffer, "rb") as wf:
        audio_data = np.frombuffer(wf.readframes(wf.getnframes()), dtype=np.int16)

    # å…¥åŠ›ãŒæ¥µç«¯ã«çŸ­ã„å ´åˆã¯å‡¦ç†ã‚’ã‚¹ã‚­ãƒƒãƒ—
    if len(audio_data) < 1600 * 6:  # ç´„0.6ç§’æœªæº€ã®ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¹ã‚­ãƒƒãƒ—
        print("âš ï¸ éŸ³å£°ãƒ‡ãƒ¼ã‚¿ãŒçŸ­ã™ãã‚‹ãŸã‚ã€ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
        return None

    # ãƒ¢ãƒ‡ãƒ«ã«å…¥åŠ›ã§ãã‚‹å½¢å¼ã«å¤‰æ›
    audio_tensor = torch.tensor(audio_data, dtype=torch.float32).unsqueeze(0) / 32768.0

    # Embeddingï¼ˆç‰¹å¾´é‡ï¼‰ã‚’å–å¾—
    with torch.no_grad():
        embedding = embedding_model.encode_batch(audio_tensor).squeeze(0)

    return embedding


def register_reference_speaker(speaker_name, reference_audio_path):
    """ å‚è€ƒéŸ³å£°ã‹ã‚‰è©±è€…åŸ‹ã‚è¾¼ã¿ã‚’ä½œæˆã—ç™»éŒ² """
    reference_embedding = extract_embedding(reference_audio_path)
    known_speakers[speaker_name] = reference_embedding


def identify_speaker(audio_buffer):
    """è©±è€…è­˜åˆ¥ï¼šæ–°ã—ã„è©±è€…ãªã‚‰ç™»éŒ²ã€æ—¢å­˜ã®è©±è€…ãªã‚‰ãƒ©ãƒ™ãƒ«ä»˜ã‘"""
    import realtime_communicator as rc
    global known_speakers
    robot_utterance_remain = rc.get_robot_count()
    print(f"ãƒ­ãƒœãƒƒãƒˆç™ºè©±æ®‹ã‚Š: {robot_utterance_remain}")

    new_embedding = extract_embedding(audio_buffer)
    if new_embedding is None:
        return "æœªè­˜åˆ¥"

    new_embedding = new_embedding.flatten()

    best_match = None
    best_score = -1.0

    # å‚è€ƒéŸ³å£°ã‚’ä½¿ã£ã¦è©±è€…ã‚’ç‰¹å®š
    for speaker, emb in known_speakers.items():
        # ã‚«ã‚¦ãƒ³ã‚¿ãƒ¼ãŒ 0 ãªã‚‰ãƒ­ãƒœãƒƒãƒˆãƒ¢ãƒ‡ãƒ«ã¯ã‚¹ã‚­ãƒƒãƒ—
        if speaker == "ãƒ­ãƒœãƒƒãƒˆ" and robot_utterance_remain <= 0:
            continue

        similarity = 1 - cosine(new_embedding, emb.flatten())
        print(f"ğŸ” é¡ä¼¼åº¦ï¼ˆ{speaker}ï¼‰: {similarity:.2f}")
        if similarity > SIMILARITY_THRESHOLD and similarity > best_score:  # SIMILARITY_THRESHOLD ä»¥ä¸‹ãªã‚‰æ–°è¦ç™»éŒ²ã€‚ä»¥ä¸Šãªã‚‰æ—¢å­˜è©±è€…ã‹ã‚‰é¡ä¼¼åº¦ãŒæœ€ã‚‚å¤§ãã„ã‚‚ã®ã‚’é¸æŠ
            best_score = similarity
            best_match = speaker

    if best_match:
        result = best_match
    else:
        # æ–°ã—ã„è©±è€…ã¨ã—ã¦ç™»éŒ²
        new_speaker_id = f"è©±è€…_{len(known_speakers) + 1}"
        known_speakers[new_speaker_id] = new_embedding
        result = new_speaker_id

    if robot_utterance_remain > 0:
        rc.dec_robot_count()  # ãƒ­ãƒœãƒƒãƒˆç™ºè©±æ®‹ã‚Šã‚’ãƒ‡ã‚¯ãƒªãƒ¡ãƒ³ãƒˆ

    return result


def diarize_audio(audio_buffer):
    """è©±è€…åˆ†é›¢ã‚’é©ç”¨ã—ã¦ã€è©±è€…ã”ã¨ã®éŸ³å£°ã‚’åˆ†é›¢"""
    audio_buffer.seek(0)
    waveform, sample_rate = torchaudio.load(audio_buffer)
    diarization = diarization_pipeline({"waveform": waveform, "sample_rate": sample_rate}, min_speakers=MIN_SPEAKERS, max_speakers=MAX_SPEAKERS)
    # diarization = diarization_pipeline({"waveform": waveform, "sample_rate": sample_rate}, num_speakers=NUM_SPEAKERS)

    # ğŸ”¹ ç™ºè©±é †ã‚’ä¿æŒã™ã‚‹ãŸã‚ã€ãƒªã‚¹ãƒˆã‚’ä½¿ç”¨
    speaker_timeline = []

    for turn, _, speaker in diarization.itertracks(yield_label=True):
        speaker_timeline.append((speaker, turn.start, turn.end))

    return speaker_timeline, waveform, sample_rate


def record_audio():
    """
    éŸ³å£°éŒ²éŸ³ã‚¹ãƒ¬ãƒƒãƒ‰
    éŸ³å£°ãŒç„¡éŸ³ã«ãªã£ãŸã‚‰ã€éŸ³å£°ã‚’ã‚­ãƒ¥ãƒ¼ã«è¿½åŠ 
    """
    print("ğŸ”´ éŒ²éŸ³é–‹å§‹...")
    p = pyaudio.PyAudio()
    stream = p.open(format=FORMAT, channels=CHANNELS, rate=SAMPLE_RATE, input=True, frames_per_buffer=CHUNK)

    frames = []
    silence_start_time = None
    has_voice = False

    try:
        while True:
            if not recording_enabled:
                time.sleep(0.1)
                continue
            try:
                data = stream.read(CHUNK, exception_on_overflow=False)
            except IOError:
                print("âš ï¸ éŸ³å£°ãƒ‡ãƒ¼ã‚¿ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚ç„¡éŸ³ã¨ã—ã¦å‡¦ç†ã—ã¾ã™ã€‚")
                data = b"\x00" * CHUNK

            frames.append(data)

            if is_speech(data, SAMPLE_RATE):
                has_voice = True
                silence_start_time = None
            else:
                if silence_start_time is None:
                    silence_start_time = time.time()
                elif time.time() - silence_start_time >= SILENCE_DURATION:
                    if has_voice:
                        # print("æ²ˆé»™ã‚’æ¤œçŸ¥ï¼éŸ³å£°ã‚’ã‚­ãƒ¥ãƒ¼ã«è¿½åŠ ã—ã¾ã™...")
                        audio_queue.put(frames)
                    frames = []
                    has_voice = False
    except KeyboardInterrupt:
        print("éŒ²éŸ³çµ‚äº†")

    stream.stop_stream()
    stream.close()
    p.terminate()


def process_audio():
    """
    éŸ³å£°å‡¦ç†ã‚¹ãƒ¬ãƒƒãƒ‰
    éŸ³å£°ãŒã‚­ãƒ¥ãƒ¼ã«è¿½åŠ ã•ã‚Œã‚‹ãŸã³ã«ã€è©±è€…åˆ†é›¢ã¨æ–‡å­—èµ·ã“ã—ã‚’å®Ÿè¡Œ
    """
    global buffer_speaker, buffer_text, buffer_time
    while True:
        frames = audio_queue.get()
        if not frames:
            continue

        print("éŸ³å£°å‡¦ç†ä¸­...")
        audio_buffer = BytesIO()
        with wave.open(audio_buffer, "wb") as wf:
            wf.setnchannels(CHANNELS)
            wf.setsampwidth(2)
            wf.setframerate(SAMPLE_RATE)
            wf.writeframes(b"".join(frames))
        audio_buffer.seek(0)

        speaker_timeline, waveform, sample_rate = diarize_audio(audio_buffer)

        prev_speaker = None
        combined_audio_list = []
        current_audio = BytesIO()

        for speaker, start, end in speaker_timeline:
            start_sample = int(start * sample_rate)
            end_sample = int(end * sample_rate)
            segment_waveform = waveform[:, start_sample:end_sample]

            # ğŸ”¹ åŒã˜è©±è€…ã®ç™ºè©±ãªã‚‰éŸ³å£°ã‚’çµåˆ
            if prev_speaker == speaker:
                combined_audio_list.append(segment_waveform.numpy())
            else:
                # ğŸ”¹ è©±è€…ãŒå¤‰ã‚ã£ãŸã‚‰ã€ç›´å‰ã®è©±è€…ã®éŸ³å£°ã‚’å‡¦ç†ã€‚1äººã®å ´åˆã¯å…¥ã‚‰ãªã„
                if combined_audio_list:
                    combined_waveform = np.concatenate(combined_audio_list, axis=1)
                    sf.write(current_audio, combined_waveform.T, sample_rate, format="WAV")
                    current_audio.seek(0)

                    recognized_speaker = identify_speaker(current_audio)
                    if recognized_speaker == "æœªè­˜åˆ¥":
                        print("âš ï¸ è©±è€…ãŒè­˜åˆ¥ã§ããªã‹ã£ãŸãŸã‚ã€æ–‡å­—èµ·ã“ã—ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
                    elif recognized_speaker == "ãƒ­ãƒœãƒƒãƒˆ":
                        print("ğŸ¤– ãƒ­ãƒœãƒƒãƒˆã®ç™ºè©±ã®ãŸã‚ã€ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
                    else:
                        transcript = client.audio.transcriptions.create(
                            # model="whisper-1",
                            # model="gpt-4o-mini-transcribe",  # é€Ÿåº¦é‡è¦–
                            model="gpt-4o-transcribe",
                            file=("audio_segment.wav", current_audio, "audio/wav"),
                            language="ja"
                        )
                        if not is_japanese(transcript.text):
                            print(f"âš ï¸ è©±è€…è­˜åˆ¥çµæœãŒæ—¥æœ¬èªã§ã¯ã‚ã‚Šã¾ã›ã‚“: {transcript.text}")
                            continue
                        print(f"[{recognized_speaker}] {transcript.text}")
                        timestamp = datetime.now()
                        # éŸ³å£°èªè­˜å¾Œã«ã€ä¸€ã¤å‰ã¨è©±è€…ãŒåŒã˜ãªã‚‰çµåˆã—ã¦ã€ç™ºè©±æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
                        if buffer_speaker == recognized_speaker:
                            # åŒä¸€è©±è€…ãªã‚‰è¿½è¨˜
                            buffer_text += " " + transcript.text
                        else:
                            # è©±è€…ãŒå¤‰ã‚ã£ãŸã‚‰ã€ã¾ãšå‰ã®ãƒãƒƒãƒ•ã‚¡ã‚’ãƒ•ãƒ©ãƒƒã‚·ãƒ¥
                            if buffer_speaker is not None:
                                send_conversation(buffer_speaker, buffer_text)  # ç™ºè©±ãƒ­ã‚°ã‚’ãƒ–ãƒ©ã‚¦ã‚¶ã¸é€ä¿¡
                                session_manager.add_utterance_count({
                                    "time": buffer_time,
                                    "speaker": buffer_speaker,
                                    "utterance": buffer_text
                                })
                                log_line = f"[{buffer_time.strftime('%Y-%m-%d %H:%M:%S')}] [{buffer_speaker}] {buffer_text}"
                                conversation_log.append(log_line)
                            # æ–°ã—ã„ãƒãƒƒãƒ•ã‚¡ã‚’é–‹å§‹
                            buffer_speaker = recognized_speaker
                            buffer_text = transcript.text
                            buffer_time = timestamp

                # ğŸ”¹ æ–°ã—ã„è©±è€…ã®ãŸã‚ã«ãƒªã‚»ãƒƒãƒˆ
                combined_audio_list = [segment_waveform.numpy()]
                current_audio = BytesIO()

            prev_speaker = speaker

        # ğŸ”¹ æœ€å¾Œã®è©±è€…ã®ç™ºè©±ã‚‚å‡¦ç†ã€‚1äººã®å ´åˆã¯ã“ã£ã¡ã«å…¥ã‚‹
        if combined_audio_list:
            combined_waveform = np.concatenate(combined_audio_list, axis=1)
            sf.write(current_audio, combined_waveform.T, sample_rate, format="WAV")
            current_audio.seek(0)

            recognized_speaker = identify_speaker(current_audio)
            if recognized_speaker == "æœªè­˜åˆ¥":
                print("âš ï¸ è©±è€…ãŒè­˜åˆ¥ã§ããªã‹ã£ãŸãŸã‚ã€æ–‡å­—èµ·ã“ã—ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
            elif recognized_speaker == "ãƒ­ãƒœãƒƒãƒˆ":
                print("ğŸ¤– ãƒ­ãƒœãƒƒãƒˆã®ç™ºè©±ã®ãŸã‚ã€ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
            else:
                transcript = client.audio.transcriptions.create(
                    # model="whisper-1",
                    # model="gpt-4o-mini-transcribe",  # é€Ÿåº¦é‡è¦–
                    model="gpt-4o-transcribe",
                    file=("audio_segment.wav", current_audio, "audio/wav"),
                    language="ja"
                )
                if not is_japanese(transcript.text):
                    print(f"âš ï¸ è©±è€…è­˜åˆ¥çµæœãŒæ—¥æœ¬èªã§ã¯ã‚ã‚Šã¾ã›ã‚“: {transcript.text}")
                    continue
                print(f"[{recognized_speaker}] {transcript.text}")
                timestamp = datetime.now()
                # éŸ³å£°èªè­˜å¾Œã«ã€ä¸€ã¤å‰ã¨è©±è€…ãŒåŒã˜ãªã‚‰çµåˆã—ã¦ã€ç™ºè©±æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
                if buffer_speaker == recognized_speaker:
                    print("åŒä¸€è©±è€…ã®ç™ºè©±ã‚’æ¤œå‡º")
                    # åŒä¸€è©±è€…ãªã‚‰è¿½è¨˜
                    buffer_text += " " + transcript.text
                else:
                    # è©±è€…ãŒå¤‰ã‚ã£ãŸã‚‰ã€ã¾ãšå‰ã®ãƒãƒƒãƒ•ã‚¡ã‚’ãƒ•ãƒ©ãƒƒã‚·ãƒ¥
                    if buffer_speaker is not None:
                        print(f"ãƒ•ãƒ©ãƒƒã‚·ãƒ¥: {buffer_speaker} - {buffer_text}")
                        send_conversation(buffer_speaker, buffer_text)  # ç™ºè©±ãƒ­ã‚°ã‚’ãƒ–ãƒ©ã‚¦ã‚¶ã¸é€ä¿¡
                        session_manager.add_utterance_count({
                            "time": buffer_time,
                            "speaker": buffer_speaker,
                            "utterance": buffer_text
                        })
                        log_line = f"[{buffer_time.strftime('%Y-%m-%d %H:%M:%S')}] [{buffer_speaker}] {buffer_text}"
                        conversation_log.append(log_line)
                    # æ–°ã—ã„ãƒãƒƒãƒ•ã‚¡ã‚’é–‹å§‹
                    print(f"æ–°ã—ã„ãƒãƒƒãƒ•ã‚¡ã‚’é–‹å§‹: {recognized_speaker}")
                    buffer_speaker = recognized_speaker
                    buffer_text = transcript.text
                    buffer_time = timestamp


def process_audio_batch():
    """
    éŸ³å£°å‡¦ç†ã‚¹ãƒ¬ãƒƒãƒ‰
    éŸ³å£°ãŒã‚­ãƒ¥ãƒ¼ã«è¿½åŠ ã•ã‚Œã‚‹ãŸã³ã«ã€è©±è€…åˆ†é›¢ã¨æ–‡å­—èµ·ã“ã—ã‚’å®Ÿè¡Œ
    """
    utterance_buffer = []
    while True:
        frames = audio_queue.get()
        if not frames:
            continue

        print("éŸ³å£°å‡¦ç†ä¸­...")
        audio_buffer = BytesIO()
        with wave.open(audio_buffer, "wb") as wf:
            wf.setnchannels(CHANNELS)
            wf.setsampwidth(2)
            wf.setframerate(SAMPLE_RATE)
            wf.writeframes(b"".join(frames))
        audio_buffer.seek(0)

        speaker_timeline, waveform, sample_rate = diarize_audio(audio_buffer)

        prev_speaker = None
        combined_audio_list = []
        current_audio = BytesIO()

        for speaker, start, end in speaker_timeline:
            start_sample = int(start * sample_rate)
            end_sample = int(end * sample_rate)
            segment_waveform = waveform[:, start_sample:end_sample]

            # ğŸ”¹ åŒã˜è©±è€…ã®ç™ºè©±ãªã‚‰éŸ³å£°ã‚’çµåˆ
            if prev_speaker == speaker:
                combined_audio_list.append(segment_waveform.numpy())
            else:
                # ğŸ”¹ è©±è€…ãŒå¤‰ã‚ã£ãŸã‚‰ã€ç›´å‰ã®è©±è€…ã®éŸ³å£°ã‚’å‡¦ç†
                if combined_audio_list:
                    combined_waveform = np.concatenate(combined_audio_list, axis=1)
                    sf.write(current_audio, combined_waveform.T, sample_rate, format="WAV")
                    current_audio.seek(0)

                    recognized_speaker = identify_speaker(current_audio)
                    if recognized_speaker == "æœªè­˜åˆ¥":
                        print("âš ï¸ è©±è€…ãŒè­˜åˆ¥ã§ããªã‹ã£ãŸãŸã‚ã€æ–‡å­—èµ·ã“ã—ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
                    elif recognized_speaker == "ãƒ­ãƒœãƒƒãƒˆ":
                        print("ğŸ¤– ãƒ­ãƒœãƒƒãƒˆã®ç™ºè©±ã®ãŸã‚ã€ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
                    else:
                        transcript = client.audio.transcriptions.create(
                            # model="whisper-1",
                            # model="gpt-4o-mini-transcribe",  # é€Ÿåº¦é‡è¦–
                            model="gpt-4o-transcribe",
                            file=("audio_segment.wav", current_audio, "audio/wav"),
                            language="ja"
                        )
                        if not is_japanese(transcript.text):
                            print(f"âš ï¸ è©±è€…è­˜åˆ¥çµæœãŒæ—¥æœ¬èªã§ã¯ã‚ã‚Šã¾ã›ã‚“: {transcript.text}")
                            continue
                        print(f"[{recognized_speaker}] {transcript.text}")
                        send_conversation(recognized_speaker, transcript.text)  # ç™ºè©±ãƒ­ã‚°ã‚’ãƒ–ãƒ©ã‚¦ã‚¶ã¸é€ä¿¡
                        timestamp = datetime.now()
                        # SessionManagerã«ç™ºè©±ãƒ­ã‚°ã¨ã—ã¦è¿½åŠ 
                        log_data = {
                            "time": timestamp,
                            "speaker": recognized_speaker,
                            "utterance": transcript.text
                        }
                        # ãƒãƒƒãƒ•ã‚¡ã«æºœã‚ã‚‹
                        utterance_buffer.append(log_data)
                        # N_BATCHãŸã¾ã£ãŸã‚‰éåŒæœŸã‚­ãƒ¥ãƒ¼ã¸æŠ•å…¥
                        if len(utterance_buffer) >= N_BATCH:
                            session_manager.batch_queue.put(utterance_buffer.copy())
                            utterance_buffer.clear()
                        log_line = f"[{timestamp.strftime('%Y-%m-%d %H:%M:%S')}] [{recognized_speaker}] {transcript.text}"
                        conversation_log.append(log_line)

                # ğŸ”¹ æ–°ã—ã„è©±è€…ã®ãŸã‚ã«ãƒªã‚»ãƒƒãƒˆ
                combined_audio_list = [segment_waveform.numpy()]
                current_audio = BytesIO()

            prev_speaker = speaker

        # ğŸ”¹ æœ€å¾Œã®è©±è€…ã®ç™ºè©±ã‚‚å‡¦ç†
        if combined_audio_list:
            combined_waveform = np.concatenate(combined_audio_list, axis=1)
            sf.write(current_audio, combined_waveform.T, sample_rate, format="WAV")
            current_audio.seek(0)

            recognized_speaker = identify_speaker(current_audio)
            if recognized_speaker == "æœªè­˜åˆ¥":
                print("âš ï¸ è©±è€…ãŒè­˜åˆ¥ã§ããªã‹ã£ãŸãŸã‚ã€æ–‡å­—èµ·ã“ã—ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
            elif recognized_speaker == "ãƒ­ãƒœãƒƒãƒˆ":
                print("ğŸ¤– ãƒ­ãƒœãƒƒãƒˆã®ç™ºè©±ã®ãŸã‚ã€ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
            else:
                transcript = client.audio.transcriptions.create(
                    # model="whisper-1",
                    # model="gpt-4o-mini-transcribe",  # é€Ÿåº¦é‡è¦–
                    model="gpt-4o-transcribe",
                    file=("audio_segment.wav", current_audio, "audio/wav"),
                    language="ja"
                )
                if not is_japanese(transcript.text):
                    print(f"âš ï¸ è©±è€…è­˜åˆ¥çµæœãŒæ—¥æœ¬èªã§ã¯ã‚ã‚Šã¾ã›ã‚“: {transcript.text}")
                    continue
                print(f"[{recognized_speaker}] {transcript.text}")
                send_conversation(recognized_speaker, transcript.text)  # ç™ºè©±ãƒ­ã‚°ã‚’ãƒ–ãƒ©ã‚¦ã‚¶ã¸é€ä¿¡
                timestamp = datetime.now()
                # SessionManagerã«ç™ºè©±ãƒ­ã‚°ã¨ã—ã¦è¿½åŠ 
                log_data = {
                    "time": timestamp,
                    "speaker": recognized_speaker,
                    "utterance": transcript.text
                }
                # ãƒãƒƒãƒ•ã‚¡ã«æºœã‚ã‚‹
                utterance_buffer.append(log_data)
                # N_BATCHãŸã¾ã£ãŸã‚‰éåŒæœŸã‚­ãƒ¥ãƒ¼ã¸æŠ•å…¥
                if len(utterance_buffer) >= N_BATCH:
                    session_manager.batch_queue.put(utterance_buffer.copy())
                    utterance_buffer.clear()
                log_line = f"[{timestamp.strftime('%Y-%m-%d %H:%M:%S')}] [{recognized_speaker}] {transcript.text}"
                conversation_log.append(log_line)


def save_conversation_log():
    if not conversation_log:
        print("ğŸ”• ä¼šè©±ãƒ­ã‚°ã¯ç©ºã§ã™ã€‚")
        return

    os.makedirs("logs", exist_ok=True)
    filename = datetime.now().strftime("logs/conversation7.txt")
    with open(filename, "w", encoding="utf-8") as f:
        for line in conversation_log:
            f.write(line + "\n")
    print(f"ğŸ’¾ ä¼šè©±ãƒ­ã‚°ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {filename}")


# ç™ºè©±ãƒ­ã‚°ã‚’ãƒ–ãƒ©ã‚¦ã‚¶ã¸é€ä¿¡
def send_conversation(speaker, utterance):
    if socketio_cli.connected:
        socketio_cli.emit("conversation_update", {
            "speaker": speaker,
            "utterance": utterance
        })


@socketio_cli.on("robot_log")
def on_conversation_update(data):
    speaker = data.get("speaker")
    utterance = data.get("utterance")
    timestamp = datetime.now()
    log_line = f"[{timestamp.strftime('%Y-%m-%d %H:%M:%S')}] [{speaker}] {utterance}"
    conversation_log.append(log_line)
    print("ğŸ¤– ãƒ­ãƒœãƒƒãƒˆã®ãƒ­ã‚°è¨˜éŒ²")


try_connect_socketio()
if __name__ == "__main__":
    register_reference_speaker("å°é‡å¯º", "static/audio/onodera_sample.wav")
    # register_reference_speaker("ä»Šäº•", "static/audio/imai_sample.wav")
    register_reference_speaker("ä½è—¤", "static/audio/sato_sample.wav")
    register_reference_speaker("ç”°ä¸­", "static/audio/tanaka_sample.wav")
    register_reference_speaker("ãƒ­ãƒœãƒƒãƒˆ", "static/audio/robot_sample.wav")
    # register_reference_speaker("å¤§å ´", "static/audio/oba_sample.wav")
    # register_reference_speaker("é¦¬å ´", "static/audio/hibiki_sample.wav")
    # register_reference_speaker("ä¸‰å®…", "static/audio/serina_sample.wav")
    # while True:
    #     record_and_transcribe()
    threading.Thread(target=record_audio, daemon=True).start()
    threading.Thread(target=process_audio, daemon=True).start()
    # threading.Thread(target=process_audio_batch, daemon=True).start()

    try:
        while True:
            time.sleep(1)
    except KeyboardInterrupt:
        print("ğŸ›‘ çµ‚äº†ä¿¡å·ã‚’å—ã‘å–ã‚Šã¾ã—ãŸã€‚ãƒ­ã‚°ã‚’ä¿å­˜ã—ã¾ã™...")
        session_manager.finalize()  # æœ€å¾Œã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’ã‚­ãƒ¥ãƒ¼ã«é€ã‚‹
        save_conversation_log()

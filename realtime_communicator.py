import os
import re
import pyaudio
import wave
import time
import queue
import threading
from io import BytesIO
from openai import OpenAI
import webrtcvad
import torch
import numpy as np
from pyannote.audio import Pipeline
from speechbrain.inference import SpeakerRecognition
from huggingface_hub import login
from scipy.spatial.distance import cosine
from dotenv import load_dotenv
import torchaudio
import soundfile as sf
from datetime import datetime
from session_manager import SessionManager
import socketio
from google.cloud.speech_v2 import SpeechClient
from google.cloud.speech_v2.types import cloud_speech
from google.api_core.client_options import ClientOptions
from typing import List
from google.cloud import speech
import sys
import config

load_dotenv()

socketio_cli = socketio.Client()

client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
# Hugging Face ã«ãƒ­ã‚°ã‚¤ãƒ³
login(token=os.getenv("HUGGINGFACE_TOKEN"))
# Google Cloud Speech-to-Text ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆID
PROJECT_ID = os.getenv("GOOGLE_CLOUD_PROJECT")
RECOGNIZER = f"projects/{PROJECT_ID}/locations/eu/recognizers/my-recognizer"

# è©±è€…è­˜åˆ¥ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰
diarization_pipeline = Pipeline.from_pretrained("pyannote/speaker-diarization-3.1", use_auth_token=os.getenv("HUGGINGFACE_TOKEN"))

# GPU ã‚’ä½¿ç”¨ã§ãã‚‹ãªã‚‰ä½¿ç”¨
device = torch.device('mps') if torch.backends.mps.is_available() else torch.device('cpu')
print(f"Using device: {device}, GPUã‚’ä½¿ã†ã‹? {torch.backends.mps.is_available()}")
diarization_pipeline.to(device)

embedding_model = SpeakerRecognition.from_hparams(source="speechbrain/spkrec-ecapa-voxceleb", savedir="tmp")

# æ—¥æœ¬èªæ–‡å­—ï¼ˆã²ã‚‰ãŒãªãƒ»ã‚«ã‚¿ã‚«ãƒŠãƒ»CJKçµ±åˆæ¼¢å­—ï¼‰ã®æ­£è¦è¡¨ç¾
_jp_regex = re.compile(r'[\u3040-\u309F\u30A0-\u30FF\u4E00-\u9FFF]')

# éŒ²éŸ³è¨­å®š
SAMPLE_RATE = 16000  # Whisperã®æ¨å¥¨ã‚µãƒ³ãƒ—ãƒ«ãƒ¬ãƒ¼ãƒˆ
CHUNK = 160  # 16-bit PCM (1ã‚µãƒ³ãƒ—ãƒ«=2ãƒã‚¤ãƒˆ) â†’ 160ã‚µãƒ³ãƒ—ãƒ« = 320ãƒã‚¤ãƒˆ
FORMAT = pyaudio.paInt16  # 16-bit PCM
CHANNELS = 1  # ãƒ¢ãƒãƒ©ãƒ«
VAD_MODE = 3  # 0(æ•æ„Ÿ) - 3(éˆæ„Ÿ) (webrtcVADã§æ²ˆé»™æ¤œçŸ¥)
SILENCE_DURATION = 0.5  # ç„¡éŸ³ãŒç¶šã„ãŸã‚‰é€ä¿¡ã™ã‚‹ç§’æ•°
MIN_SPEAKERS = 1  # æœ€å°ã®è©±è€…æ•°
MAX_SPEAKERS = 4  # æœ€å¤§ã®è©±è€…æ•°
# NUM_SPEAKERS = 3  # ç¢ºå®šè©±è€…æ•°
SIMILARITY_THRESHOLD = -10  # æ—¢å­˜ã®è©±è€…ã¨ã®é¡ä¼¼åº¦ã®é–¾å€¤
N_BATCH = 5  # ã‚»ãƒƒã‚·ãƒ§ãƒ³åˆ†å‰²åˆ¤å®šã®ãƒãƒƒãƒã‚µã‚¤ã‚º
ROBOT_UTTERANCE_REMAIN = 0  # ãƒ­ãƒœãƒƒãƒˆç™ºè©±å¾Œã«ãƒ­ãƒœãƒƒãƒˆè­˜åˆ¥ã‚’æœ‰åŠ¹ã«ã™ã‚‹æ®‹ã‚Šç™ºè©±æ•°
USE_GOOGLE_STT = "v1"  # "v1": v1ã®éã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã€"v1-streaming": v1ã®ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã€ "v2-streaming": v2ã®ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã€False: OpenAI
USE_DIRECT_STREAM = False  # True ã«ã™ã‚‹ã¨ãƒã‚¤ã‚¯ãƒãƒ£ãƒ³ã‚¯ã‚’ç›´æ¥STTã¸æµã—è¾¼ã‚€
DIARIZATION_THRESHOLD = 5  # è©±è€…åˆ†é›¢ã™ã‚‹ã‹ã©ã†ã‹ã®é–¾å€¤
SKIP_THRESHOLD_BYTES = 30000  # éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã®ãƒã‚¤ãƒˆæ•°ãŒã“ã®å€¤ä»¥ä¸‹ãªã‚‰å‡¦ç†ã‚’ã‚¹ã‚­ãƒƒãƒ—

# éŸ³å£°ã‚¢ã‚¯ãƒ†ã‚£ãƒ“ãƒ†ã‚£æ¤œå‡º (VAD)
vad = webrtcvad.Vad(VAD_MODE)

# æ—¢å­˜ã®è©±è€…æƒ…å ± (è©±è€…å: embedding)
known_speakers = {}

# éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã®ã‚­ãƒ¥ãƒ¼ï¼ˆéŒ²éŸ³ã‚¹ãƒ¬ãƒƒãƒ‰ â†’ å‡¦ç†ã‚¹ãƒ¬ãƒƒãƒ‰ã¸æ¸¡ã™ï¼‰
audio_queue = queue.Queue()

# ä¼šè©±å±¥æ­´ãƒ­ã‚°
conversation_log = []

stt_requests_queue = queue.Queue()
stream_buffer: List[bytes] = []

# === ã‚»ãƒƒã‚·ãƒ§ãƒ³ç®¡ç†ã®åˆæœŸåŒ– ===
session_manager = SessionManager()

recording_enabled = True

# ãƒ†ã‚­ã‚¹ãƒˆçµåˆç”¨ã®å¤‰æ•°
buffer_speaker = None
buffer_text = ""
buffer_time = None

# æ¨™æº–å‡ºåŠ›ãƒ»ã‚¨ãƒ©ãƒ¼ã‚’ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã«ãƒªãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆ
original_stdout = sys.stdout
original_stderr = sys.stderr

# ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é–‹ã
log_path = os.path.join(config.LOG_ROOT, "terminal.txt")
terminal_log = open(log_path, "w", encoding="utf-8")


# ä¸¡æ–¹ã®ã‚¹ãƒˆãƒªãƒ¼ãƒ ã«æ›¸ãè¾¼ã‚€ Tee ã‚¯ãƒ©ã‚¹
class Tee:
    def __init__(self, *streams):
        self.streams = streams

    def write(self, data):
        for s in self.streams:
            s.write(data)

    def flush(self):
        for s in self.streams:
            s.flush()


sys.stdout = Tee(original_stdout, terminal_log)
sys.stderr = Tee(original_stderr, terminal_log)


# recording_enabledã‚’ã‚¹ã‚¿ãƒ¼ãƒˆãƒœã‚¿ãƒ³æŠ¼ã—ãŸã‚‰Trueã€ã‚¹ãƒˆãƒƒãƒ—ãƒœã‚¿ãƒ³æŠ¼ã—ãŸã‚‰Falseã«ã™ã‚‹
@socketio_cli.on("control")
def on_control(data):
    global recording_enabled
    recording_enabled = (data.get("action") == "start")
    print(f"ğŸ”” control: recording_enabled = {recording_enabled}")


def set_robot_count(n):
    global ROBOT_UTTERANCE_REMAIN
    ROBOT_UTTERANCE_REMAIN = n


def dec_robot_count():
    global ROBOT_UTTERANCE_REMAIN
    if ROBOT_UTTERANCE_REMAIN > 0:
        ROBOT_UTTERANCE_REMAIN -= 1


def get_robot_count():
    return ROBOT_UTTERANCE_REMAIN


def is_japanese(text: str) -> bool:
    return bool(_jp_regex.search(text))


def try_connect_socketio(url="http://localhost:8888", max_retries=10, interval_sec=2):
    """Flask Socket.IO ã‚µãƒ¼ãƒã¸ã®æ¥ç¶šã‚’ãƒªãƒˆãƒ©ã‚¤ã—ãªãŒã‚‰è©¦ã¿ã‚‹"""
    for attempt in range(1, max_retries + 1):
        try:
            socketio_cli.connect(url)
            print("âœ… Socket.IO ã«æ¥ç¶šã—ã¾ã—ãŸ")
            return
        except Exception as e:
            print(f"âš ï¸ Socket.IO æ¥ç¶šå¤±æ•— (è©¦è¡Œ {attempt}/{max_retries}): {e}")
            time.sleep(interval_sec)
    print("âŒ Socket.IO ã«æ¥ç¶šã§ãã¾ã›ã‚“ã§ã—ãŸã€‚Web UI æ©Ÿèƒ½ã¯ç„¡åŠ¹ã«ãªã‚Šã¾ã™ã€‚")


def transcribe_streaming_v2(stream_file: str) -> cloud_speech.StreamingRecognizeResponse:
    """ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªãƒ•ã‚¡ã‚¤ãƒ«ã‚¹ãƒˆãƒªãƒ¼ãƒ ã‹ã‚‰ Google Cloud Speech-to-Text API ã‚’ä½¿ç”¨ã—ã¦éŸ³å£°ã‚’æ–‡å­—èµ·ã“ã—
    Args:
        stream_file (str): æ–‡å­—èµ·ã“ã—å¯¾è±¡ã®ãƒ­ãƒ¼ã‚«ãƒ«ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªãƒ•ã‚¡ã‚¤ãƒ«ã¸ã®ãƒ‘ã‚¹ã€‚
        ä¾‹: "resources/audio.wav"
    Returns:
        list[cloud_speech.StreamingRecognizeResponse]:
        å„ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªã‚»ã‚°ãƒ¡ãƒ³ãƒˆã«å¯¾å¿œã™ã‚‹æ–‡å­—èµ·ã“ã—çµæœã‚’å«ã‚€ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®ãƒªã‚¹ãƒˆã€‚
    """
    # ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
    client = SpeechClient(client_options=ClientOptions(api_endpoint="eu-speech.googleapis.com"))

    # ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒã‚¤ãƒˆåˆ—ã¨ã—ã¦èª­ã¿è¾¼ã‚€
    with open(stream_file, "rb") as f:
        audio_content = f.read()

    # âš ï¸ v2 ã® audio chunk ã¯æœ€å¤§ 25600 bytes ã¾ã§
    MAX_BYTES = 25600
    stream = [
        audio_content[i : i + MAX_BYTES]
        for i in range(0, len(audio_content), MAX_BYTES)
    ]

    audio_requests = (
        cloud_speech.StreamingRecognizeRequest(audio=audio) for audio in stream
    )

    recognition_config = cloud_speech.RecognitionConfig(
        auto_decoding_config=cloud_speech.AutoDetectDecodingConfig(),
        language_codes=["ja-JP"],
        model="long",
        features=cloud_speech.RecognitionFeatures(enable_automatic_punctuation=True),  # å¥èª­ç‚¹æŒ¿å…¥
    )
    streaming_config = cloud_speech.StreamingRecognitionConfig(config=recognition_config)
    config_request = cloud_speech.StreamingRecognizeRequest(
        recognizer=RECOGNIZER,
        streaming_config=streaming_config,
    )

    def requests(config: cloud_speech.RecognitionConfig, audio: list) -> list:
        yield config
        yield from audio

    # éŸ³å£°ã‚’ãƒ†ã‚­ã‚¹ãƒˆã«æ–‡å­—èµ·ã“ã—
    try:
        responses_iterator = client.streaming_recognize(
            requests=requests(config_request, audio_requests)
        )
    except Exception as e:
        print(f"âš ï¸ Google Cloud Speech-to-Text ã‚¨ãƒ©ãƒ¼: {e}")
        return []
    responses = []
    for response in responses_iterator:
        responses.append(response)

    return responses


def transcribe_streaming_v1(stream_file: str) -> list[speech.StreamingRecognizeResponse]:
    """v1 API ã§ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚“ã§ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°èªè­˜ã‚’è¡Œã„ã€StreamingRecognizeResponse ã®ãƒªã‚¹ãƒˆã‚’è¿”ã™"""
    # ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
    client = speech.SpeechClient()

    # ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒã‚¤ãƒˆåˆ—ã¨ã—ã¦èª­ã¿è¾¼ã‚€
    with open(stream_file, "rb") as f:
        audio_content = f.read()

    # v1 ã§ã¯å¤§ãã™ãã‚‹ãƒãƒ£ãƒ³ã‚¯ã‚’é¿ã‘ã‚‹ç¨‹åº¦ã«ç´„32KBã”ã¨ã«åˆ‡ã‚Šå‡ºã—
    CHUNK_BYTES = 32000
    chunks = [
        audio_content[i : i + CHUNK_BYTES]
        for i in range(0, len(audio_content), CHUNK_BYTES)
    ]
    audio_requests = (
        speech.StreamingRecognizeRequest(audio_content=chunk)
        for chunk in chunks
    )

    config = speech.RecognitionConfig(
        encoding=speech.RecognitionConfig.AudioEncoding.LINEAR16,
        sample_rate_hertz=SAMPLE_RATE,
        language_code="ja-JP",
        enable_automatic_punctuation=True,
    )
    streaming_config = speech.StreamingRecognitionConfig(
        config=config,
        interim_results=False,
    )

    # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°èªè­˜ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿
    def requests_gen():
        yield from audio_requests

    # èªè­˜å®Ÿè¡Œï¼†çµæœåé›†
    responses: list[speech.StreamingRecognizeResponse] = []
    try:
        for resp in client.streaming_recognize(streaming_config, requests_gen()):
            responses.append(resp)
    except Exception as e:
        print(f"âš ï¸ v1 ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°èªè­˜ã‚¨ãƒ©ãƒ¼: {e}")
    return responses


def transcribe_v1(stream_file: str) -> speech.RecognizeResponse:
    """ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªãƒ•ã‚¡ã‚¤ãƒ«ã‚’ Google Cloud Speech-to-Text v1 ã® éã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚° Recognize API ã§æ–‡å­—èµ·ã“ã—"""
    # ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
    client = speech.SpeechClient()

    # ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒã‚¤ãƒˆåˆ—ã¨ã—ã¦èª­ã¿è¾¼ã‚€
    with open(stream_file, "rb") as f:
        content = f.read()

    # èªè­˜è¨­å®š (LINEAR16, 16kHz, æ—¥æœ¬èª)
    config = speech.RecognitionConfig(
        encoding=speech.RecognitionConfig.AudioEncoding.LINEAR16,
        sample_rate_hertz=SAMPLE_RATE,
        language_code="ja-JP",
        enable_automatic_punctuation=True,
    )
    audio = speech.RecognitionAudio(content=content)

    # éã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°èªè­˜ã‚’å®Ÿè¡Œã—ã¦çµæœã‚’è¿”ã™
    return client.recognize(config=config, audio=audio)


def is_speech(frame, sample_rate):
    """VAD ã‚’ä½¿ã£ã¦éŸ³å£°ã‹ç„¡éŸ³ã‹ã‚’åˆ¤å®š"""
    if len(frame) not in [160, 320, 480]:  # ãƒ•ãƒ¬ãƒ¼ãƒ ã‚µã‚¤ã‚ºã‚’ãƒã‚§ãƒƒã‚¯
        print(f"âš ï¸ ãƒ•ãƒ¬ãƒ¼ãƒ ã‚µã‚¤ã‚ºãŒç•°å¸¸: {len(frame)} ãƒã‚¤ãƒˆ (è¨±å®¹å€¤: 160, 320, 480)")
        return False
    return vad.is_speech(frame, sample_rate)


def extract_embedding(audio_input):
    """éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰è©±è€…ã®ç‰¹å¾´é‡ï¼ˆembeddingï¼‰ã‚’å–å¾—"""

    if isinstance(audio_input, str):  # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã®å ´åˆ
        with open(audio_input, "rb") as f:
            audio_buffer = BytesIO(f.read())
    else:  # æ—¢ã« `BytesIO` ã®å ´åˆ
        audio_buffer = audio_input

    audio_buffer.seek(0)

    with wave.open(audio_buffer, "rb") as wf:
        audio_data = np.frombuffer(wf.readframes(wf.getnframes()), dtype=np.int16)

    # å…¥åŠ›ãŒæ¥µç«¯ã«çŸ­ã„å ´åˆã¯å‡¦ç†ã‚’ã‚¹ã‚­ãƒƒãƒ—
    if len(audio_data) < 1600 * 6:  # ç´„0.6ç§’æœªæº€ã®ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¹ã‚­ãƒƒãƒ—
        print("âš ï¸ éŸ³å£°ãƒ‡ãƒ¼ã‚¿ãŒçŸ­ã„ï¼šè©±è€…è­˜åˆ¥ã‚¹ã‚­ãƒƒãƒ—")
        return None

    # ãƒ¢ãƒ‡ãƒ«ã«å…¥åŠ›ã§ãã‚‹å½¢å¼ã«å¤‰æ›
    audio_tensor = torch.tensor(audio_data, dtype=torch.float32).unsqueeze(0) / 32768.0

    # Embeddingï¼ˆç‰¹å¾´é‡ï¼‰ã‚’å–å¾—
    with torch.no_grad():
        embedding = embedding_model.encode_batch(audio_tensor).squeeze(0)

    return embedding


def register_reference_speaker(speaker_name, reference_audio_path):
    """ å‚è€ƒéŸ³å£°ã‹ã‚‰è©±è€…åŸ‹ã‚è¾¼ã¿ã‚’ä½œæˆã—ç™»éŒ² """
    reference_embedding = extract_embedding(reference_audio_path)
    known_speakers[speaker_name] = reference_embedding


def identify_speaker(audio_buffer):
    """è©±è€…è­˜åˆ¥ï¼šæ–°ã—ã„è©±è€…ãªã‚‰ç™»éŒ²ã€æ—¢å­˜ã®è©±è€…ãªã‚‰ãƒ©ãƒ™ãƒ«ä»˜ã‘"""
    import realtime_communicator as rc
    global known_speakers
    robot_utterance_remain = rc.get_robot_count()
    print(f"ãƒ­ãƒœãƒƒãƒˆç™ºè©±æ®‹ã‚Š: {robot_utterance_remain}")

    new_embedding = extract_embedding(audio_buffer)
    if new_embedding is None:
        return "æœªè­˜åˆ¥"

    new_embedding = new_embedding.flatten()

    best_match = None
    best_score = -1.0

    # å‚è€ƒéŸ³å£°ã‚’ä½¿ã£ã¦è©±è€…ã‚’ç‰¹å®š
    for speaker, emb in known_speakers.items():
        # ã‚«ã‚¦ãƒ³ã‚¿ãƒ¼ãŒ 0 ãªã‚‰ãƒ­ãƒœãƒƒãƒˆãƒ¢ãƒ‡ãƒ«ã¯ã‚¹ã‚­ãƒƒãƒ—
        if speaker == "ãƒ­ãƒœãƒƒãƒˆ" and robot_utterance_remain <= 0:
            continue

        similarity = 1 - cosine(new_embedding, emb.flatten())
        print(f"ğŸ” é¡ä¼¼åº¦ï¼ˆ{speaker}ï¼‰: {similarity:.2f}")
        if similarity > SIMILARITY_THRESHOLD and similarity > best_score:  # SIMILARITY_THRESHOLD ä»¥ä¸‹ãªã‚‰æ–°è¦ç™»éŒ²ã€‚ä»¥ä¸Šãªã‚‰æ—¢å­˜è©±è€…ã‹ã‚‰é¡ä¼¼åº¦ãŒæœ€ã‚‚å¤§ãã„ã‚‚ã®ã‚’é¸æŠ
            best_score = similarity
            best_match = speaker

    if best_match:
        result = best_match
    else:
        # æ–°ã—ã„è©±è€…ã¨ã—ã¦ç™»éŒ²
        new_speaker_id = f"è©±è€…_{len(known_speakers) + 1}"
        known_speakers[new_speaker_id] = new_embedding
        result = new_speaker_id

    if robot_utterance_remain > 0:
        rc.dec_robot_count()  # ãƒ­ãƒœãƒƒãƒˆç™ºè©±æ®‹ã‚Šã‚’ãƒ‡ã‚¯ãƒªãƒ¡ãƒ³ãƒˆ

    return result


def diarize_audio(audio_buffer):
    """è©±è€…åˆ†é›¢ã‚’é©ç”¨ã—ã¦ã€è©±è€…ã”ã¨ã®éŸ³å£°ã‚’åˆ†é›¢"""
    print(f"éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã®è©±è€…ã”ã¨åˆ†é›¢é–‹å§‹{datetime.now()}")
    audio_buffer.seek(0)
    waveform, sample_rate = torchaudio.load(audio_buffer)
    diarization = diarization_pipeline({"waveform": waveform, "sample_rate": sample_rate}, min_speakers=MIN_SPEAKERS, max_speakers=MAX_SPEAKERS)
    # diarization = diarization_pipeline({"waveform": waveform, "sample_rate": sample_rate}, num_speakers=NUM_SPEAKERS)

    # ğŸ”¹ ç™ºè©±é †ã‚’ä¿æŒã™ã‚‹ãŸã‚ã€ãƒªã‚¹ãƒˆã‚’ä½¿ç”¨
    speaker_timeline = []

    for turn, _, speaker in diarization.itertracks(yield_label=True):
        speaker_timeline.append((speaker, turn.start, turn.end))

    print(f"éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã®è©±è€…ã”ã¨åˆ†é›¢çµ‚äº†{datetime.now()}")
    return speaker_timeline, waveform, sample_rate


def record_audio():
    """
    éŸ³å£°éŒ²éŸ³ã‚¹ãƒ¬ãƒƒãƒ‰
    éŸ³å£°ãŒç„¡éŸ³ã«ãªã£ãŸã‚‰ã€éŸ³å£°ã‚’ã‚­ãƒ¥ãƒ¼ã«è¿½åŠ 
    """
    print("ğŸ”´ éŒ²éŸ³é–‹å§‹...")
    p = pyaudio.PyAudio()
    stream = p.open(format=FORMAT, channels=CHANNELS, rate=SAMPLE_RATE, input=True, frames_per_buffer=CHUNK)

    frames = []
    silence_start_time = None
    has_voice = False

    try:
        while True:
            if not recording_enabled:
                time.sleep(0.1)
                continue
            try:
                data = stream.read(CHUNK, exception_on_overflow=False)
            except IOError:
                print("âš ï¸ éŸ³å£°ãƒ‡ãƒ¼ã‚¿ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚ç„¡éŸ³ã¨ã—ã¦å‡¦ç†ã—ã¾ã™ã€‚")
                data = b"\x00" * CHUNK

            # VAD ã§ã»ã¼ç„¡éŸ³ãªã‚‰å®Œå…¨ã‚¹ã‚­ãƒƒãƒ—
            # if not is_speech(data, SAMPLE_RATE):
            #     continue

            frames.append(data)

            if USE_DIRECT_STREAM:
                stt_requests_queue.put(data)
                stream_buffer.append(data)
            else:
                if is_speech(data, SAMPLE_RATE):
                    has_voice = True
                    silence_start_time = None
                else:
                    if silence_start_time is None:
                        silence_start_time = time.time()
                    elif time.time() - silence_start_time >= SILENCE_DURATION:
                        if has_voice:
                            # print("æ²ˆé»™ã‚’æ¤œçŸ¥ï¼éŸ³å£°ã‚’ã‚­ãƒ¥ãƒ¼ã«è¿½åŠ ã—ã¾ã™...")
                            audio_queue.put(frames)
                        frames = []
                        has_voice = False
    except KeyboardInterrupt:
        print("éŒ²éŸ³çµ‚äº†")

    stream.stop_stream()
    stream.close()
    p.terminate()


def process_audio():
    """
    éŸ³å£°å‡¦ç†ã‚¹ãƒ¬ãƒƒãƒ‰
    éŸ³å£°ãŒã‚­ãƒ¥ãƒ¼ã«è¿½åŠ ã•ã‚Œã‚‹ãŸã³ã«ã€è©±è€…åˆ†é›¢ã¨æ–‡å­—èµ·ã“ã—ã‚’å®Ÿè¡Œ
    """
    global buffer_speaker, buffer_text, buffer_time
    while True:
        frames = audio_queue.get()
        if not frames:
            continue

        print(f"éŸ³å£°å‡¦ç†é–‹å§‹ï¼š{datetime.now()}")
        audio_buffer = BytesIO()
        with wave.open(audio_buffer, "wb") as wf:
            wf.setnchannels(CHANNELS)
            wf.setsampwidth(2)
            wf.setframerate(SAMPLE_RATE)
            wf.writeframes(b"".join(frames))
        audio_buffer.seek(0)

        # â”€â”€ DIARIZATION_THRESHOLD ç§’æœªæº€ã¯è©±è€…åˆ†é›¢ã›ãšã€Œ1 äººã€ã¨è¦‹ãªã™ â”€â”€
        with wave.open(audio_buffer, 'rb') as wf2:
            nframes = wf2.getnframes()
            framerate = wf2.getframerate()
            duration = nframes / framerate
        audio_buffer.seek(0)
        if duration >= DIARIZATION_THRESHOLD:
            speaker_timeline, waveform, sample_rate = diarize_audio(audio_buffer)
        else:
            # çŸ­ã„éŸ³å£°ã¯ã€Œ1 äººã€æ‰±ã„
            waveform, sample_rate = torchaudio.load(audio_buffer)
            speaker_timeline = [('single', 0.0, duration)]

        # éŸ³å£°ãŒã‚ã‚‹ãƒã‚¤ãƒˆæ•°ã‚ˆã‚Šã‚‚å°ã•ã‘ã‚Œã°ã€å‡¦ç†ã‚’å…¨ã¦ã‚¹ã‚­ãƒƒãƒ—
        # print(f"éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã®ãƒã‚¤ãƒˆæ•°: {audio_buffer.getbuffer().nbytes}")
        if audio_buffer.getbuffer().nbytes < SKIP_THRESHOLD_BYTES:
            print("âš ï¸ éŸ³å£°ãƒ‡ãƒ¼ã‚¿ãŒçŸ­ã„ï¼šå‡¦ç†ã‚’å…¨ã¦ã‚¹ã‚­ãƒƒãƒ—")
            continue

        prev_speaker = None
        combined_audio_list = []
        current_audio = BytesIO()

        for speaker, start, end in speaker_timeline:
            start_sample = int(start * sample_rate)
            end_sample = int(end * sample_rate)
            segment_waveform = waveform[:, start_sample:end_sample]

            # ğŸ”¹ åŒã˜è©±è€…ã®ç™ºè©±ãªã‚‰éŸ³å£°ã‚’çµåˆ
            if prev_speaker == speaker:
                combined_audio_list.append(segment_waveform.numpy())
            else:
                # ğŸ”¹ è©±è€…ãŒå¤‰ã‚ã£ãŸã‚‰ã€ç›´å‰ã®è©±è€…ã®éŸ³å£°ã‚’å‡¦ç†ã€‚1äººã®å ´åˆã¯å…¥ã‚‰ãªã„
                if combined_audio_list:
                    combined_waveform = np.concatenate(combined_audio_list, axis=1)
                    try:
                        sf.write(current_audio, combined_waveform.T, sample_rate, format="WAV")
                        current_audio.seek(0)
                    except Exception as e:
                        print(f"âš ï¸ WAV æ›¸ãè¾¼ã¿ã‚¨ãƒ©ãƒ¼ï¼ˆè¤‡æ•°ã‚»ã‚°ãƒ¡ãƒ³ãƒˆï¼‰ï¼š{e}")
                        # ã“ã®ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã®å‡¦ç†ã‚’ã‚¹ã‚­ãƒƒãƒ—
                        combined_audio_list = []
                        continue

                    # print(f"è©±è€…è­˜åˆ¥é–‹å§‹ï¼ˆè¤‡æ•°ï¼‰ï¼š{datetime.now()}")
                    recognized_speaker = identify_speaker(current_audio)
                    # print(f"è©±è€…è­˜åˆ¥çµ‚äº†ï¼ˆè¤‡æ•°ï¼‰ï¼š{datetime.now()}")
                    if recognized_speaker == "æœªè­˜åˆ¥":
                        print("âš ï¸ è©±è€…ãŒè­˜åˆ¥ã§ããªã‹ã£ãŸãŸã‚ã€æ–‡å­—èµ·ã“ã—ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
                    elif recognized_speaker == "ãƒ­ãƒœãƒƒãƒˆ":
                        print("ğŸ¤– ãƒ­ãƒœãƒƒãƒˆã®ç™ºè©±ã®ãŸã‚ã€ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
                    else:
                        print(f"éŸ³å£°èªè­˜é–‹å§‹ï¼ˆè¤‡æ•°ï¼‰ï¼š{datetime.now()}")
                        if USE_GOOGLE_STT:
                            # Google Cloud Speech-to-Text v2 ã‚’ä½¿ã†
                            # --- ãƒãƒƒãƒ•ã‚¡ã‚’æ›¸ãå‡ºã—ã¦ãƒ•ã‚¡ã‚¤ãƒ«åŒ– ---
                            tmp_path = "audio_segment.wav"
                            with open(tmp_path, "wb") as wf:
                                wf.write(current_audio.getbuffer())
                            if USE_GOOGLE_STT == "v2-streaming":
                                responses = transcribe_streaming_v2(tmp_path)
                            elif USE_GOOGLE_STT == "v1-streaming":
                                responses = transcribe_streaming_v1(tmp_path)
                            elif USE_GOOGLE_STT == "v1":
                                response = transcribe_v1(tmp_path)
                                transcript_text = "".join(
                                    result.alternatives[0].transcript
                                    for result in response.results
                                )
                            if USE_GOOGLE_STT == "v1-streaming" or USE_GOOGLE_STT == "v2-streaming":
                                # å„ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®æœ€åˆã®ä»£æ›¿å€™è£œã‚’å–ã‚Šå‡ºã—ã¦çµåˆ
                                transcript_text = "".join(
                                    res.results[0].alternatives[0].transcript
                                    for res in responses
                                    if res.results
                                )
                        else:
                            # gpt-4o-transcribe ã‚’ä½¿ã†
                            resp = client.audio.transcriptions.create(
                                # model="whisper-1",
                                # model="gpt-4o-mini-transcribe",  # é€Ÿåº¦é‡è¦–
                                model="gpt-4o-transcribe",
                                file=("audio_segment.wav", current_audio, "audio/wav"),
                                language="ja"
                            )
                            transcript_text = resp.text
                        print(f"éŸ³å£°èªè­˜çµ‚äº†ï¼ˆè¤‡æ•°ï¼‰ï¼š{datetime.now()}")
                        if not is_japanese(transcript_text):
                            print(f"âš ï¸ è©±è€…è­˜åˆ¥çµæœãŒæ—¥æœ¬èªã§ã¯ã‚ã‚Šã¾ã›ã‚“: {transcript_text}")
                            continue
                        print(f"ğŸ§‘[{recognized_speaker}] {transcript_text}")
                        timestamp = datetime.now()
                        # éŸ³å£°èªè­˜å¾Œã«ã€ä¸€ã¤å‰ã¨è©±è€…ãŒåŒã˜ãªã‚‰çµåˆã—ã¦ã€ç™ºè©±æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
                        if buffer_speaker == recognized_speaker:
                            # åŒä¸€è©±è€…ãªã‚‰è¿½è¨˜
                            buffer_text += " " + transcript_text
                        else:
                            # è©±è€…ãŒå¤‰ã‚ã£ãŸã‚‰ã€ã¾ãšå‰ã®ãƒãƒƒãƒ•ã‚¡ã‚’ãƒ•ãƒ©ãƒƒã‚·ãƒ¥
                            if buffer_speaker is not None:
                                send_conversation(buffer_speaker, buffer_text)  # ç™ºè©±ãƒ­ã‚°ã‚’ãƒ–ãƒ©ã‚¦ã‚¶ã¸é€ä¿¡
                                session_manager.add_utterance_count({
                                    "time": buffer_time,
                                    "speaker": buffer_speaker,
                                    "utterance": buffer_text
                                })
                                log_line = f"[{buffer_time.strftime('%Y-%m-%d %H:%M:%S')}] [{buffer_speaker}] {buffer_text}"
                                conversation_log.append(log_line)
                            # æ–°ã—ã„ãƒãƒƒãƒ•ã‚¡ã‚’é–‹å§‹
                            buffer_speaker = recognized_speaker
                            buffer_text = transcript_text
                            buffer_time = timestamp

                # ğŸ”¹ æ–°ã—ã„è©±è€…ã®ãŸã‚ã«ãƒªã‚»ãƒƒãƒˆ
                combined_audio_list = [segment_waveform.numpy()]
                current_audio = BytesIO()

            prev_speaker = speaker

        # ğŸ”¹ æœ€å¾Œã®è©±è€…ã®ç™ºè©±ã‚‚å‡¦ç†ã€‚1äººã®å ´åˆã¯ã“ã£ã¡ã«å…¥ã‚‹
        if combined_audio_list:
            combined_waveform = np.concatenate(combined_audio_list, axis=1)
            try:
                sf.write(current_audio, combined_waveform.T, sample_rate, format="WAV")
                current_audio.seek(0)
            except Exception as e:
                print(f"âš ï¸ WAV æ›¸ãè¾¼ã¿ã‚¨ãƒ©ãƒ¼ï¼ˆæœ€çµ‚ã‚»ã‚°ãƒ¡ãƒ³ãƒˆï¼‰ï¼š{e}")
                # æœ€çµ‚ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã ã‘ã‚¹ã‚­ãƒƒãƒ—ã—ã¦çµ‚äº†
                continue

            # print(f"è©±è€…è­˜åˆ¥é–‹å§‹ï¼ˆ1äººï¼‰ï¼š{datetime.now()}")
            recognized_speaker = identify_speaker(current_audio)
            # print(f"è©±è€…è­˜åˆ¥çµ‚äº†ï¼ˆ1äººï¼‰ï¼š{datetime.now()}")
            if recognized_speaker == "æœªè­˜åˆ¥":
                print("âš ï¸ è©±è€…ãŒè­˜åˆ¥ã§ããªã‹ã£ãŸãŸã‚ã€æ–‡å­—èµ·ã“ã—ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
            elif recognized_speaker == "ãƒ­ãƒœãƒƒãƒˆ":
                print("ğŸ¤– ãƒ­ãƒœãƒƒãƒˆã®ç™ºè©±ã®ãŸã‚ã€ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
            else:
                print(f"éŸ³å£°èªè­˜é–‹å§‹ï¼ˆ1äººï¼‰ï¼š{datetime.now()}")
                if USE_GOOGLE_STT:
                    # Google Cloud Speech-to-Text v2 ã‚’ä½¿ã†
                    # --- ãƒãƒƒãƒ•ã‚¡ã‚’æ›¸ãå‡ºã—ã¦ãƒ•ã‚¡ã‚¤ãƒ«åŒ– ---
                    tmp_path = "audio_segment.wav"
                    with open(tmp_path, "wb") as wf:
                        wf.write(current_audio.getbuffer())
                    if USE_GOOGLE_STT == "v2-streaming":
                        responses = transcribe_streaming_v2(tmp_path)
                    elif USE_GOOGLE_STT == "v1-streaming":
                        responses = transcribe_streaming_v1(tmp_path)
                    elif USE_GOOGLE_STT == "v1":
                        response = transcribe_v1(tmp_path)
                        transcript_text = "".join(
                            result.alternatives[0].transcript
                            for result in response.results
                        )
                    if USE_GOOGLE_STT == "v1-streaming" or USE_GOOGLE_STT == "v2-streaming":
                        # å„ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®æœ€åˆã®ä»£æ›¿å€™è£œã‚’å–ã‚Šå‡ºã—ã¦çµåˆ
                        transcript_text = "".join(
                            res.results[0].alternatives[0].transcript
                            for res in responses
                            if res.results
                        )
                else:
                    # gpt-4o-transcribe ã‚’ä½¿ã†
                    resp = client.audio.transcriptions.create(
                        # model="whisper-1",
                        # model="gpt-4o-mini-transcribe",  # é€Ÿåº¦é‡è¦–
                        model="gpt-4o-transcribe",
                        file=("audio_segment.wav", current_audio, "audio/wav"),
                        language="ja"
                    )
                    transcript_text = resp.text
                print(f"éŸ³å£°èªè­˜çµ‚äº†ï¼ˆ1äººï¼‰ï¼š{datetime.now()}")
                if not is_japanese(transcript_text):
                    print(f"âš ï¸ éŸ³å£°èªè­˜çµæœãŒæ—¥æœ¬èªã§ã¯ã‚ã‚Šã¾ã›ã‚“: {transcript_text}")
                    continue
                print(f"ğŸ§‘[{recognized_speaker}] {transcript_text}")
                timestamp = datetime.now()
                # éŸ³å£°èªè­˜å¾Œã«ã€ä¸€ã¤å‰ã¨è©±è€…ãŒåŒã˜ãªã‚‰çµåˆã—ã¦ã€ç™ºè©±æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
                if buffer_speaker == recognized_speaker:
                    print("åŒä¸€è©±è€…ã®ç™ºè©±ã‚’æ¤œå‡º")
                    # åŒä¸€è©±è€…ãªã‚‰è¿½è¨˜
                    buffer_text += " " + transcript_text
                else:
                    # è©±è€…ãŒå¤‰ã‚ã£ãŸã‚‰ã€ã¾ãšå‰ã®ãƒãƒƒãƒ•ã‚¡ã‚’ãƒ•ãƒ©ãƒƒã‚·ãƒ¥
                    if buffer_speaker is not None:
                        print(f"ãƒ•ãƒ©ãƒƒã‚·ãƒ¥: {buffer_speaker} - {buffer_text}")
                        send_conversation(buffer_speaker, buffer_text)  # ç™ºè©±ãƒ­ã‚°ã‚’ãƒ–ãƒ©ã‚¦ã‚¶ã¸é€ä¿¡
                        session_manager.add_utterance_count({
                            "time": buffer_time,
                            "speaker": buffer_speaker,
                            "utterance": buffer_text
                        })
                        log_line = f"[{buffer_time.strftime('%Y-%m-%d %H:%M:%S')}] [{buffer_speaker}] {buffer_text}"
                        conversation_log.append(log_line)
                    # æ–°ã—ã„ãƒãƒƒãƒ•ã‚¡ã‚’é–‹å§‹
                    print(f"æ–°ã—ã„ãƒãƒƒãƒ•ã‚¡ã‚’é–‹å§‹: {recognized_speaker}")
                    buffer_speaker = recognized_speaker
                    buffer_text = transcript_text
                    buffer_time = timestamp
                print(f"éŸ³å£°å‡¦ç†çµ‚äº†ï¼š{datetime.now()}")


def process_audio_batch():
    """
    éŸ³å£°å‡¦ç†ã‚¹ãƒ¬ãƒƒãƒ‰
    éŸ³å£°ãŒã‚­ãƒ¥ãƒ¼ã«è¿½åŠ ã•ã‚Œã‚‹ãŸã³ã«ã€è©±è€…åˆ†é›¢ã¨æ–‡å­—èµ·ã“ã—ã‚’å®Ÿè¡Œ
    """
    global buffer_speaker, buffer_text, buffer_time
    utterance_buffer = []
    while True:
        frames = audio_queue.get()
        if not frames:
            continue

        print("éŸ³å£°å‡¦ç†ä¸­...")
        audio_buffer = BytesIO()
        with wave.open(audio_buffer, "wb") as wf:
            wf.setnchannels(CHANNELS)
            wf.setsampwidth(2)
            wf.setframerate(SAMPLE_RATE)
            wf.writeframes(b"".join(frames))
        audio_buffer.seek(0)

        speaker_timeline, waveform, sample_rate = diarize_audio(audio_buffer)

        prev_speaker = None
        combined_audio_list = []
        current_audio = BytesIO()

        for speaker, start, end in speaker_timeline:
            start_sample = int(start * sample_rate)
            end_sample = int(end * sample_rate)
            segment_waveform = waveform[:, start_sample:end_sample]

            # ğŸ”¹ åŒã˜è©±è€…ã®ç™ºè©±ãªã‚‰éŸ³å£°ã‚’çµåˆ
            if prev_speaker == speaker:
                combined_audio_list.append(segment_waveform.numpy())
            else:
                # ğŸ”¹ è©±è€…ãŒå¤‰ã‚ã£ãŸã‚‰ã€ç›´å‰ã®è©±è€…ã®éŸ³å£°ã‚’å‡¦ç†
                if combined_audio_list:
                    combined_waveform = np.concatenate(combined_audio_list, axis=1)
                    sf.write(current_audio, combined_waveform.T, sample_rate, format="WAV")
                    current_audio.seek(0)

                    recognized_speaker = identify_speaker(current_audio)
                    if recognized_speaker == "æœªè­˜åˆ¥":
                        print("âš ï¸ è©±è€…ãŒè­˜åˆ¥ã§ããªã‹ã£ãŸãŸã‚ã€æ–‡å­—èµ·ã“ã—ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
                    elif recognized_speaker == "ãƒ­ãƒœãƒƒãƒˆ":
                        print("ğŸ¤– ãƒ­ãƒœãƒƒãƒˆã®ç™ºè©±ã®ãŸã‚ã€ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
                    else:
                        transcript = client.audio.transcriptions.create(
                            # model="whisper-1",
                            # model="gpt-4o-mini-transcribe",  # é€Ÿåº¦é‡è¦–
                            model="gpt-4o-transcribe",
                            file=("audio_segment.wav", current_audio, "audio/wav"),
                            language="ja"
                        )
                        if not is_japanese(transcript.text):
                            print(f"âš ï¸ è©±è€…è­˜åˆ¥çµæœãŒæ—¥æœ¬èªã§ã¯ã‚ã‚Šã¾ã›ã‚“: {transcript.text}")
                            continue
                        print(f"[{recognized_speaker}] {transcript.text}")
                        timestamp = datetime.now()
                        # éŸ³å£°èªè­˜å¾Œã«ã€ä¸€ã¤å‰ã¨è©±è€…ãŒåŒã˜ãªã‚‰çµåˆã—ã¦ã€ç™ºè©±æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
                        if buffer_speaker == recognized_speaker:
                            # åŒä¸€è©±è€…ãªã‚‰è¿½è¨˜
                            buffer_text += " " + transcript.text
                        else:
                            # è©±è€…ãŒå¤‰ã‚ã£ãŸã‚‰ã€ã¾ãšå‰ã®ãƒãƒƒãƒ•ã‚¡ã‚’ãƒ•ãƒ©ãƒƒã‚·ãƒ¥
                            if buffer_speaker is not None:
                                send_conversation(buffer_speaker, buffer_text)  # ç™ºè©±ãƒ­ã‚°ã‚’ãƒ–ãƒ©ã‚¦ã‚¶ã¸é€ä¿¡
                                utterance_buffer.append({
                                    "time": buffer_time,
                                    "speaker": buffer_speaker,
                                    "utterance": buffer_text
                                })
                                log_line = f"[{buffer_time.strftime('%Y-%m-%d %H:%M:%S')}] [{buffer_speaker}] {buffer_text}"
                                conversation_log.append(log_line)
                            # æ–°ã—ã„ãƒãƒƒãƒ•ã‚¡ã‚’é–‹å§‹
                            buffer_speaker = recognized_speaker
                            buffer_text = transcript.text
                            buffer_time = timestamp
                        # N_BATCHãŸã¾ã£ãŸã‚‰éåŒæœŸã‚­ãƒ¥ãƒ¼ã¸æŠ•å…¥
                        if len(utterance_buffer) >= N_BATCH:
                            session_manager.batch_queue.put(utterance_buffer.copy())
                            utterance_buffer.clear()
                        log_line = f"[{timestamp.strftime('%Y-%m-%d %H:%M:%S')}] [{recognized_speaker}] {transcript.text}"
                        conversation_log.append(log_line)

                # ğŸ”¹ æ–°ã—ã„è©±è€…ã®ãŸã‚ã«ãƒªã‚»ãƒƒãƒˆ
                combined_audio_list = [segment_waveform.numpy()]
                current_audio = BytesIO()

            prev_speaker = speaker

        # ğŸ”¹ æœ€å¾Œã®è©±è€…ã®ç™ºè©±ã‚‚å‡¦ç†
        if combined_audio_list:
            combined_waveform = np.concatenate(combined_audio_list, axis=1)
            sf.write(current_audio, combined_waveform.T, sample_rate, format="WAV")
            current_audio.seek(0)

            recognized_speaker = identify_speaker(current_audio)
            if recognized_speaker == "æœªè­˜åˆ¥":
                print("âš ï¸ è©±è€…ãŒè­˜åˆ¥ã§ããªã‹ã£ãŸãŸã‚ã€æ–‡å­—èµ·ã“ã—ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
            elif recognized_speaker == "ãƒ­ãƒœãƒƒãƒˆ":
                print("ğŸ¤– ãƒ­ãƒœãƒƒãƒˆã®ç™ºè©±ã®ãŸã‚ã€ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
            else:
                transcript = client.audio.transcriptions.create(
                    # model="whisper-1",
                    # model="gpt-4o-mini-transcribe",  # é€Ÿåº¦é‡è¦–
                    model="gpt-4o-transcribe",
                    file=("audio_segment.wav", current_audio, "audio/wav"),
                    language="ja"
                )
                if not is_japanese(transcript.text):
                    print(f"âš ï¸ è©±è€…è­˜åˆ¥çµæœãŒæ—¥æœ¬èªã§ã¯ã‚ã‚Šã¾ã›ã‚“: {transcript.text}")
                    continue
                print(f"[{recognized_speaker}] {transcript.text}")
                timestamp = datetime.now()
                # éŸ³å£°èªè­˜å¾Œã«ã€ä¸€ã¤å‰ã¨è©±è€…ãŒåŒã˜ãªã‚‰çµåˆã—ã¦ã€ç™ºè©±æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
                if buffer_speaker == recognized_speaker:
                    # åŒä¸€è©±è€…ãªã‚‰è¿½è¨˜
                    buffer_text += " " + transcript.text
                else:
                    # è©±è€…ãŒå¤‰ã‚ã£ãŸã‚‰ã€ã¾ãšå‰ã®ãƒãƒƒãƒ•ã‚¡ã‚’ãƒ•ãƒ©ãƒƒã‚·ãƒ¥
                    if buffer_speaker is not None:
                        send_conversation(buffer_speaker, buffer_text)  # ç™ºè©±ãƒ­ã‚°ã‚’ãƒ–ãƒ©ã‚¦ã‚¶ã¸é€ä¿¡
                        utterance_buffer.append({
                            "time": buffer_time,
                            "speaker": buffer_speaker,
                            "utterance": buffer_text
                        })
                        log_line = f"[{buffer_time.strftime('%Y-%m-%d %H:%M:%S')}] [{buffer_speaker}] {buffer_text}"
                        conversation_log.append(log_line)
                    # æ–°ã—ã„ãƒãƒƒãƒ•ã‚¡ã‚’é–‹å§‹
                    buffer_speaker = recognized_speaker
                    buffer_text = transcript.text
                    buffer_time = timestamp
                # N_BATCHãŸã¾ã£ãŸã‚‰éåŒæœŸã‚­ãƒ¥ãƒ¼ã¸æŠ•å…¥
                if len(utterance_buffer) >= N_BATCH:
                    session_manager.batch_queue.put(utterance_buffer.copy())
                    utterance_buffer.clear()
                log_line = f"[{timestamp.strftime('%Y-%m-%d %H:%M:%S')}] [{recognized_speaker}] {transcript.text}"
                conversation_log.append(log_line)


def pcm_to_wav_bytesio(pcm_bytes: bytes,
                       sample_rate: int = 16000,
                       nchannels: int = 1,
                       sampwidth: int = 2) -> BytesIO:
    """ãƒ˜ãƒƒãƒ€ç„¡ã— PCM ã‚’ WAV ãƒã‚¤ãƒˆåˆ—ã«åŒ…ã‚€"""
    wav_buf = BytesIO()
    with wave.open(wav_buf, "wb") as wf:
        wf.setnchannels(nchannels)
        wf.setsampwidth(sampwidth)   # 16-bit PCM â†’ 2 byte
        wf.setframerate(sample_rate)
        wf.writeframes(pcm_bytes)
    wav_buf.seek(0)
    return wav_buf


def streaming_stt_worker1():
    """ãƒã‚¤ã‚¯å…¥åŠ›ãƒãƒ£ãƒ³ã‚¯ã‚’ç›´æ¥ Google STT v1 ã¸æµã—è¾¼ã¿ã€is_final ã”ã¨ã«è©±è€…åˆ†é›¢ã‚’ãƒˆãƒªã‚¬ãƒ¼"""
    client = speech.SpeechClient()       # v1 ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
    # æœ€åˆã® config ãƒªã‚¯ã‚¨ã‚¹ãƒˆ
    recog_conf = speech.RecognitionConfig(
        encoding=speech.RecognitionConfig.AudioEncoding.LINEAR16,
        sample_rate_hertz=SAMPLE_RATE,
        language_code="ja-JP",
        enable_automatic_punctuation=True,
    )
    streaming_conf = speech.StreamingRecognitionConfig(
        config=recog_conf,
        interim_results=True,
        single_utterance=True,
    )

    SILENCE_FRAMES = 50

    def requests_gen():
        silence_cnt = 0
        while True:
            chunk = stt_requests_queue.get()
            if chunk is None:
                break
            # --- ç„¡éŸ³åˆ¤å®šï¼ˆB: ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ VADï¼‰ ---
            if not chunk or not is_speech(chunk, SAMPLE_RATE):
                silence_cnt += 1
                if silence_cnt >= SILENCE_FRAMES:
                    break                           # ç„¡éŸ³ã—ãã„å€¤è¶…ãˆ â†’ ã‚¹ãƒˆãƒªãƒ¼ãƒ çµ‚äº†
                chunk = b"\x00" * 320     # 16 kHz Ã— 1ch Ã— 2B Ã— 0.01s
            else:
                silence_cnt = 0                     # ãƒªã‚»ãƒƒãƒˆ
            yield speech.StreamingRecognizeRequest(audio_content=chunk)

    while True:
        stream_start = time.time()
        responses = client.streaming_recognize(
            config=streaming_conf,
            requests=requests_gen(),
            timeout=900,
        )
        # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°èªè­˜ãƒ«ãƒ¼ãƒ—
        for resp in responses:
            # A: END_OF_SINGLE_UTTERANCE ã‚’ç¢ºå®šãƒˆãƒªã‚¬ã«
            if resp.speech_event_type == speech.StreamingRecognizeResponse.SpeechEventType.END_OF_SINGLE_UTTERANCE:
                if resp.results:
                    result = resp.results[-1]        # ç›´è¿‘ã®çµæœ
                    result.is_final = True           # ãƒ•ãƒ©ã‚°æ“¬ä¼¼çš„ã«ç«‹ã¦ã‚‹
                else:
                    break                            # çµæœãªã—ã§çµ‚äº†
            for result in resp.results:
                print(result)
                if result.is_final:
                    print("is_finalã«å…¥ã‚Šã¾ã—ãŸ")
                    # ç¢ºå®šåŒºé–“ã®ãƒ†ã‚­ã‚¹ãƒˆã¨ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’å–å¾—
                    text = result.alternatives[0].transcript
                    elapsed = result.result_end_time.total_seconds()
                    end = stream_start + elapsed
                    # ãƒãƒƒãƒ•ã‚¡ã‹ã‚‰ç¢ºå®šåŒºé–“ã®éŸ³å£°ã‚’åˆ‡ã‚Šå‡ºã—
                    segment = b"".join(stream_buffer)  # ãƒãƒƒãƒ•ã‚¡å†…ã®éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’çµåˆ
                    full_buf = pcm_to_wav_bytesio(segment)
                    # è©±è€…åˆ†é›¢ï¼†è©±è€…èªè­˜
                    speaker_tl, _, _ = diarize_audio(full_buf)  # è¦æ¤œè¨ã€‚çµ¶å¯¾2äººå…¥ã‚‰ãªã„ãªã‚‰ã‚³ãƒ¡ãƒ³ãƒˆ
                    speaker = identify_speaker(full_buf)
                    # ãƒ­ã‚°ï¼†ã‚»ãƒƒã‚·ãƒ§ãƒ³ç®¡ç†
                    timestamp = datetime.fromtimestamp(end)
                    send_conversation(speaker, text)
                    session_manager.add_utterance({
                        "time": timestamp,
                        "speaker": speaker,
                        "utterance": text
                    })
                    print(f"ğŸ§‘[{speaker}] {text}")
                    log_line = f"[{timestamp.strftime('%Y-%m-%d %H:%M:%S')}] [{speaker}] {text}"
                    conversation_log.append(log_line)

                    # æ¬¡åŒºé–“ç”¨ã«ãƒãƒƒãƒ•ã‚¡ã‚¯ãƒªã‚¢
                    stream_buffer.clear()


def streaming_stt_worker2():
    """ãƒã‚¤ã‚¯å…¥åŠ›ãƒãƒ£ãƒ³ã‚¯ã‚’ç›´æ¥ Google STT v2 ã¸æµã—è¾¼ã¿ã€is_final ã”ã¨ã«è©±è€…åˆ†é›¢ã‚’ãƒˆãƒªã‚¬ãƒ¼"""
    client = SpeechClient(client_options=ClientOptions(api_endpoint="eu-speech.googleapis.com"))
    # æœ€åˆã® config ãƒªã‚¯ã‚¨ã‚¹ãƒˆ
    recog_conf = cloud_speech.RecognitionConfig(
        auto_decoding_config=cloud_speech.AutoDetectDecodingConfig(),
        language_codes=["ja-JP"], model="latest_short",
        features=cloud_speech.RecognitionFeatures(enable_automatic_punctuation=True),
    )
    streaming_conf = cloud_speech.StreamingRecognitionConfig(
        config=recog_conf,
        # â”€â”€â”€â”€â”€ ã‚¹ãƒˆãƒªãƒ¼ãƒ ç¶­æŒç”¨ã®å„ç¨®ãƒ•ãƒ©ã‚° â”€â”€â”€â”€â”€â”€
        streaming_features=cloud_speech.StreamingRecognitionFeatures(
            interim_results=True,                  # éƒ¨åˆ†çµæœã‚’æ—©ã‚ã«è¿”ã™
            enable_voice_activity_events=True,     # VAD ã‚¤ãƒ™ãƒ³ãƒˆã‚‚å—ä¿¡
            # voice_activity_timeout=cloud_speech.VoiceActivityTimeout(
            #     start_timeout=duration_pb2.Duration(seconds=5),   # ç™ºè©±é–‹å§‹å¾…ã¡
            #     end_timeout=duration_pb2.Duration(seconds=30),    # ç™ºè©±å¾Œã®ç„¡éŸ³
            # ),
        ),
    )

    def requests_gen():
        # åˆå›: è¨­å®š + recognizer ã‚’é€ä¿¡
        yield cloud_speech.StreamingRecognizeRequest(
            recognizer=RECOGNIZER,
            streaming_config=streaming_conf,
        )
        # â‘¡ ä»¥é™: éŸ³å£° + recognizer
        while True:
            # 100 msï¼ˆç´„ 10 ãƒãƒ£ãƒ³ã‚¯ï¼‰åˆ†ã¾ã¨ã‚ã¦é€ä¿¡
            buf = []
            try:
                for _ in range(10):
                    buf.append(stt_requests_queue.get(timeout=0.02))
            except queue.Empty:
                pass
            chunk = b"".join(buf)
            if not chunk:                          # é€ã‚‹ã‚‚ã®ãŒç„¡ã‘ã‚Œã°ã‚¹ã‚­ãƒƒãƒ—
                continue
            # ä»¥é™ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆã«ã‚‚ recognizer ã‚’å«ã‚ã‚‹
            yield cloud_speech.StreamingRecognizeRequest(
                audio=chunk,
            )

    # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°èªè­˜ãƒ«ãƒ¼ãƒ—
    for resp in client.streaming_recognize(requests=requests_gen()):
        print(f"ğŸ”„ éŸ³å£°èªè­˜çµæœã‚’å—ä¿¡: {len(resp.results)} ä»¶")
        for result in resp.results:
            print(result)
            if result.is_final:
                print("is_finalã«å…¥ã‚Šã¾ã—ãŸ")
                # ç¢ºå®šåŒºé–“ã®ãƒ†ã‚­ã‚¹ãƒˆã¨ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’å–å¾—
                text = result.alternatives[0].transcript
                int_sec, dec_sec = result.result_end_time.seconds, result.result_end_time.nanos / 1e9
                end = int_sec + dec_sec
                # ãƒãƒƒãƒ•ã‚¡ã‹ã‚‰ç¢ºå®šåŒºé–“ã®éŸ³å£°ã‚’åˆ‡ã‚Šå‡ºã—
                segment = b"".join(stream_buffer)  # ãƒãƒƒãƒ•ã‚¡å†…ã®éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’çµåˆ
                full_buf = BytesIO(segment)
                # è©±è€…åˆ†é›¢ï¼†è©±è€…èªè­˜
                speaker_tl, _, _ = diarize_audio(full_buf)  # è¦æ¤œè¨ã€‚çµ¶å¯¾2äººå…¥ã‚‰ãªã„ãªã‚‰ã‚³ãƒ¡ãƒ³ãƒˆ
                speaker = identify_speaker(full_buf)
                # ãƒ­ã‚°ï¼†ã‚»ãƒƒã‚·ãƒ§ãƒ³ç®¡ç†
                timestamp = datetime.fromtimestamp(end)
                send_conversation(speaker, text)
                session_manager.add_utterance({
                    "time": timestamp,
                    "speaker": speaker,
                    "utterance": text
                })
                print(f"ğŸ§‘[{speaker}] {text}")
                log_line = f"[{timestamp.strftime('%Y-%m-%d %H:%M:%S')}] [{speaker}] {text}"
                conversation_log.append(log_line)

                # æ¬¡åŒºé–“ç”¨ã«ãƒãƒƒãƒ•ã‚¡ã‚¯ãƒªã‚¢
                stream_buffer.clear()


def save_conversation_log():
    if not conversation_log:
        print("ğŸ”• ä¼šè©±ãƒ­ã‚°ã¯ç©ºã§ã™ã€‚")
        return

    filename = os.path.join(config.LOG_ROOT, "conversation.txt")
    with open(filename, "w", encoding="utf-8") as f:
        for line in conversation_log:
            f.write(line + "\n")
    print(f"ğŸ’¾ ä¼šè©±ãƒ­ã‚°ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {filename}")


# ç™ºè©±ãƒ­ã‚°ã‚’ãƒ–ãƒ©ã‚¦ã‚¶ã¸é€ä¿¡
def send_conversation(speaker, utterance):
    if socketio_cli.connected:
        socketio_cli.emit("conversation_update", {
            "speaker": speaker,
            "utterance": utterance
        })


@socketio_cli.on("robot_log")
def on_conversation_update(data):
    speaker = data.get("speaker")
    utterance = data.get("utterance")
    timestamp = datetime.now()
    log_line = f"[{timestamp.strftime('%Y-%m-%d %H:%M:%S')}] [{speaker}] {utterance}"
    conversation_log.append(log_line)
    print("ğŸ¤– ãƒ­ãƒœãƒƒãƒˆã®ãƒ­ã‚°è¨˜éŒ²")


try_connect_socketio()
if __name__ == "__main__":
    register_reference_speaker("ãƒ­ãƒœãƒƒãƒˆ", "static/audio/robot_sample.wav")
    # register_reference_speaker("å°é‡å¯º", "static/audio/onodera_sample.wav")
    # register_reference_speaker("ä½è—¤", "static/audio/sato_sample.wav")
    # register_reference_speaker("ç”°ä¸­", "static/audio/tanaka_sample.wav")
    # register_reference_speaker("ä»Šäº•", "static/audio/imai_sample.wav")
    # register_reference_speaker("å¤§å ´", "static/audio/oba_sample.wav")
    # register_reference_speaker("é¦¬å ´", "static/audio/hibiki_sample.wav")
    register_reference_speaker("ä¸‰å®…", "static/audio/serina_sample.wav")
    # register_reference_speaker("ã‘ã‚“ã—ã‚“", "static/audio/kenshin_sample.wav")
    register_reference_speaker("ç«‹å·", "static/audio/kanta_sample.wav")
    register_reference_speaker("æ¾å´", "static/audio/matsuzaki_sample.wav")
    # register_reference_speaker("ã‘ã„ã˜ã‚ã†", "static/audio/keijiro_sample.wav")
    # register_reference_speaker("ã‚†ã†ã", "static/audio/yuki_sample.wav")
    # register_reference_speaker("ãªã‹ãã†", "static/audio/nakasou_sample.wav")
    # register_reference_speaker("ãªãŠã", "static/audio/naoki_sample.wav")
    # while True:
    #     record_and_transcribe()
    threading.Thread(target=record_audio, daemon=True).start()
    if USE_DIRECT_STREAM:
        threading.Thread(target=streaming_stt_worker2, daemon=True).start()  # ãƒã‚¤ã‚¯å…¥åŠ›ãƒãƒ£ãƒ³ã‚¯ã‚’ç›´æ¥STTã¸æµã—è¾¼
    else:
        threading.Thread(target=process_audio, daemon=True).start()  # 0.5ç§’ã”ã¨ã«éŸ³å£°ã‚’å‡¦ç†ã™ã‚‹
        # threading.Thread(target=process_audio_batch, daemon=True).start()  # ãƒãƒƒãƒå‡¦ç†ã§éŸ³å£°ã‚’å‡¦ç†ã™ã‚‹ï¼ˆä¼šè©±æ•°ã”ã¨ã«ã‚»ãƒƒã‚·ãƒ§ãƒ³åˆ†å‰²åˆ¤å®šã—ã¦ã„ã‚‹å ´åˆã¯ä½¿ã‚ãªã„ï¼‰

    try:
        while True:
            time.sleep(1)
    except KeyboardInterrupt:
        print("ğŸ›‘ çµ‚äº†ä¿¡å·ã‚’å—ã‘å–ã‚Šã¾ã—ãŸã€‚ãƒ­ã‚°ã‚’ä¿å­˜ã—ã¾ã™...")
        session_manager.finalize()  # æœ€å¾Œã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’ã‚­ãƒ¥ãƒ¼ã«é€ã‚‹
        save_conversation_log()
